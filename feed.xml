<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://logicatcore.github.io/scratchpad/feed.xml" rel="self" type="application/atom+xml" /><link href="https://logicatcore.github.io/scratchpad/" rel="alternate" type="text/html" /><updated>2020-12-16T17:18:26-06:00</updated><id>https://logicatcore.github.io/scratchpad/feed.xml</id><title type="html">Scratchpad</title><subtitle>Time well spent</subtitle><entry><title type="html">Machine Learning Optimization Algorithms</title><link href="https://logicatcore.github.io/scratchpad/2020/10/28/Machine-Learning-Optimization-Algorithms.ipynb" rel="alternate" type="text/html" title="Machine Learning Optimization Algorithms" /><published>2020-10-28T00:00:00-05:00</published><updated>2020-10-28T00:00:00-05:00</updated><id>https://logicatcore.github.io/scratchpad/2020/10/28/Machine-Learning-Optimization-Algorithms</id><author><name></name></author><summary type="html">{ &quot;cells&quot;: [ { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;# \&quot;Optimization algorithms using pytorch\&quot;\n&quot;, &quot;&gt; \&quot;Optimization algorithms comparision and 3D &amp; 2D visualisation\&quot;\n&quot;, &quot;\n&quot;, &quot;- toc: false\n&quot;, &quot;- branch: master\n&quot;, &quot;- badges: true\n&quot;, &quot;- comments: true\n&quot;, &quot;- categories: [machine learning, jupyter]\n&quot;, &quot;- image: images/optimization_algos/Figure_2.png\n&quot;, &quot;- search_exclude: false&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Optimization algorithms using pytorch\n&quot;, &quot;***&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;Optimization algorithms play a central role in the learning process of most of the machine learning and deep learning algorithms. Here are some of the well known algorithms-\n&quot;, &quot; 1. Vanilla Gradient descent\n&quot;, &quot; 2. Gradient descent with Momentum\n&quot;, &quot; 3. RMSprop\n&quot;, &quot; 4. Adam\n&quot;, &quot;\n&quot;, &quot;While all the 4 above listed algorithms differ in their own way and have certain advantages and disadvantages. They share certain similarities with the simple graddient descent algorithm. In this blog post we will go through these 4 algorithms and see how they function on minimizing the loss or finding the minima of a random error function with multiple minimas and maximas implemented using *pytorch*.\n&quot;, &quot;\n&quot;, &quot;## Error function with multiple minimas and maximas\n&quot;, &quot;***\n&quot;, &quot;*Error Function* $= f(x,y) = 3 \\times e^{(-(y + 1)^2 - x^2)} \\times (x - 1)^2 - \\frac{e^{(-(x + 1)^2 - y^2)}}{3} + e^{(-x^2 - y^2)} \\times (10x^3 - 2x + 10y^5)$\n&quot;, &quot;\n&quot;, &quot;![Error functions](/scratchpad/images/optimization_algos/Figure_2.png)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Note: In this blog post, I will not be going into the theory of all the algorithms used rather just concentrate on the implementation and the results\n&quot;, &quot;\n&quot;, &quot;For theoretical reference please refer to [d2lai chapter on optimization algorithms](https://d2l.ai/chapter_optimization/index.html)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;### Import all the necessary python modules&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 1, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;%matplotlib widget\n&quot;, &quot;\n&quot;, &quot;import torch\n&quot;, &quot;import IPython\n&quot;, &quot;import numpy as np\n&quot;, &quot;\n&quot;, &quot;from IPython import display\n&quot;, &quot;from IPython.display import HTML\n&quot;, &quot;\n&quot;, &quot;import matplotlib as mpl\n&quot;, &quot;import matplotlib.pyplot as plt\n&quot;, &quot;from matplotlib import animation\n&quot;, &quot;from mpl_toolkits.mplot3d import Axes3D\n&quot;, &quot;from mpl_toolkits.mplot3d import proj3d\n&quot;, &quot;from matplotlib.patches import FancyArrowPatch\n&quot;, &quot;\n&quot;, &quot;# mpl.rcParams['savefig.dpi'] = 300\n&quot;, &quot;plt.style.use('seaborn')&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;### To draw arrows to monitor the progress of optimization&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 2, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;# To draw 3d arrows in matplotlib\n&quot;, &quot;class Arrow3D(FancyArrowPatch):\n&quot;, &quot; def __init__(self, xs, ys, zs, *args, **kwargs):\n&quot;, &quot; FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs)\n&quot;, &quot; self._verts3d = xs, ys, zs\n&quot;, &quot;\n&quot;, &quot; def draw(self, renderer):\n&quot;, &quot; xs3d, ys3d, zs3d = self._verts3d\n&quot;, &quot; xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n&quot;, &quot; self.set_positions((xs[0], ys[0]), (xs[1], ys[1]))\n&quot;, &quot; FancyArrowPatch.draw(self, renderer)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;### Error or loss calculation based on 2 independent parameters&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 3, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def calc_z(xx, yy)-&gt; torch.tensor:\n&quot;, &quot; \&quot;\&quot;\&quot;\n&quot;, &quot; Returns the loss at a certain point\n&quot;, &quot; \&quot;\&quot;\&quot;\n&quot;, &quot; return 3 * torch.exp(-(yy + 1) ** 2 - xx ** 2) * (xx - 1) ** 2 - torch.exp(-(xx + 1) ** 2 - yy ** 2) / 3 + torch.exp(\n&quot;, &quot; -xx ** 2 - yy ** 2) * (10 * xx ** 3 - 2 * xx + 10 * yy ** 5)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 4, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;fps = 10 # frames per second - to save the progress in optimization as a video\n&quot;, &quot;Writer = animation.writers['ffmpeg']\n&quot;, &quot;writer = Writer(fps=fps, metadata=dict(artist='Me'), bitrate=1800)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Initialise the plot with the error function terrain&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 5, &quot;metadata&quot;: {}, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;application/vnd.jupyter.widget-view+json&quot;: { &quot;model_id&quot;: &quot;ab87915ed00e4ead88d74bb25c131471&quot;, &quot;version_major&quot;: 2, &quot;version_minor&quot;: 0 }, &quot;text/plain&quot;: [ &quot;Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦&quot; ] }, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;display_data&quot; }, { &quot;data&quot;: { &quot;text/plain&quot;: [ &quot;&quot; ] }, &quot;execution_count&quot;: 5, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;x = torch.linspace(-3, 3, 600)\n&quot;, &quot;y = torch.linspace(-3, 3, 600)\n&quot;, &quot;xgrid, ygrid = torch.meshgrid(x, y)\n&quot;, &quot;zgrid = calc_z(xgrid, ygrid)\n&quot;, &quot;\n&quot;, &quot;fig = plt.figure(figsize=(14,6))\n&quot;, &quot;ax0 = fig.add_subplot(121, projection='3d')\n&quot;, &quot;ax0.set_xlabel('$x$')\n&quot;, &quot;ax0.set_ylabel('$y$')\n&quot;, &quot;ax0.set_zlabel('Random Loss function: ' + '$f(x, y)$')\n&quot;, &quot;ax0.axis('auto')\n&quot;, &quot;cs = ax0.plot_surface(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), cmap='viridis', alpha=0.6)\n&quot;, &quot;fig.colorbar(cs)\n&quot;, &quot;\n&quot;, &quot;ax1 = fig.add_subplot(122)\n&quot;, &quot;qcs = ax1.contour(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), 20, cmap='viridis')\n&quot;, &quot;fig.colorbar(qcs)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;![side_by_side](/scratchpad/images/optimization_algos/sidebyside.jpeg)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Vanilla Gradient Descent\n&quot;, &quot;***\n&quot;, &quot;Gradient descent algorithm which is an iterative optimization algorithm can be described as loop which is executed repeatedly until certain convergence criteria has been met. Gradient descent can be explained using the following equation.\n&quot;, &quot;\n&quot;, &quot;### Gradient calculation\n&quot;, &quot;***\n&quot;, &quot;$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n&quot;, &quot;\\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix}$\n&quot;, &quot;\n&quot;, &quot;### Update equation\n&quot;, &quot;***\n&quot;, &quot;$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)}{\\partial (w_{x,y}^l)}$&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 6, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;epochs = 20\n&quot;, &quot;lr = 0.01 # learning rate\n&quot;, &quot;\n&quot;, &quot;xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot;\n&quot;, &quot;new_z = 0\n&quot;, &quot;dy_dx_current = 0&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def step_gd(i):\n&quot;, &quot; global dy_dx_current, xys, lr, new_z, ax0, ax1\n&quot;, &quot; if i == 0:\n&quot;, &quot; # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot; xys = torch.tensor([-0.5, -0.7], requires_grad=True) \n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; \n&quot;, &quot; dy_dx_current = xys.grad\n&quot;, &quot; \n&quot;, &quot; cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n&quot;, &quot;\n&quot;, &quot; xys = (xys - lr * dy_dx_current).clone().detach().requires_grad_(True)\n&quot;, &quot; \n&quot;, &quot; # vanilla gradient descent\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n&quot;, &quot; dy_dx_current = xys.grad\n&quot;, &quot; \n&quot;, &quot; xys_plot = xys.detach().numpy()\n&quot;, &quot; ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='r', s=20, zorder=3)\n&quot;, &quot; a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n&quot;, &quot; [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n&quot;, &quot; lw=2, arrowstyle=\&quot;-|&gt;\&quot;, color=\&quot;k\&quot;)\n&quot;, &quot; ax0.add_artist(a)\n&quot;, &quot; \n&quot;, &quot; ax1.scatter(xys_plot[0], xys_plot[1], marker='*', c='r')&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;anim_gd = animation.FuncAnimation(fig, step_gd, frames=epochs, interval=(1/fps)*1000, repeat=False)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;# HTML(anim_gd.to_html5_video())\n&quot;, &quot;anim_gd.save('gd.mp4', writer=writer)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Gradient descent with momentum\n&quot;, &quot;\n&quot;, &quot;### Gradient calculation\n&quot;, &quot;\n&quot;, &quot;$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n&quot;, &quot;\\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} = \\beta * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} + (1 - \\beta) * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial y}\n&quot;, &quot;\\end{vmatrix}$\n&quot;, &quot;\n&quot;, &quot;### Update equation\n&quot;, &quot;$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)}{\\partial (w_{x,y}^l)}$&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;epochs = 60\n&quot;, &quot;lr = 0.01 # learning rate\n&quot;, &quot;\n&quot;, &quot;xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot;\n&quot;, &quot;new_z = 0\n&quot;, &quot;dy_dx_current_gdm = 0\n&quot;, &quot;\n&quot;, &quot;dy_dx_new_gdm = torch.tensor([0.0, 0.0])&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def step_gdm(i):\n&quot;, &quot; global dy_dx_new_gdm, dy_dx_current_gdm, xys, lr, new_z, ax0, ax1\n&quot;, &quot; if i == 0:\n&quot;, &quot; # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot; xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; \n&quot;, &quot; dy_dx_current_gdm = xys.grad\n&quot;, &quot; \n&quot;, &quot; cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n&quot;, &quot; \n&quot;, &quot; dy_dx_new_gdm = 0.9*dy_dx_new_gdm + (1 - 0.9)*dy_dx_current_gdm\n&quot;, &quot; xys = (xys - lr * dy_dx_new_gdm).clone().detach().requires_grad_(True)\n&quot;, &quot; \n&quot;, &quot; # gradient descent with momentum\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n&quot;, &quot; dy_dx_current_gdm = xys.grad\n&quot;, &quot; \n&quot;, &quot; xys_plot = xys.detach().numpy()\n&quot;, &quot; ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='g', s=20, zorder=3)\n&quot;, &quot; a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n&quot;, &quot; [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n&quot;, &quot; lw=2, arrowstyle=\&quot;-|&gt;\&quot;, color=\&quot;k\&quot;)\n&quot;, &quot; ax0.add_artist(a)\n&quot;, &quot; \n&quot;, &quot; ax1.scatter(xys_plot[0], xys_plot[1], marker='*', c='g')&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;anim_gdm = animation.FuncAnimation(fig, step_gdm, frames=epochs, interval=(1/fps)*1000, repeat=False)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;# HTML(anim_gdm.to_html5_video())\n&quot;, &quot;anim_gdm.save('momentum.mp4', writer=writer)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## RMSprop\n&quot;, &quot;***\n&quot;, &quot;### Gradient calculation\n&quot;, &quot;***\n&quot;, &quot;$\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n&quot;, &quot;\\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} = \\beta * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} + (1 - \\beta) * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial y}\n&quot;, &quot;\\end{vmatrix}^2$\n&quot;, &quot;\n&quot;, &quot;### Update equation\n&quot;, &quot;***\n&quot;, &quot;$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\frac{\\partial (Error_{new})}{\\partial (w_{x,y}^l)}}{\\sqrt{\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} + \\epsilon}}$&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;epochs = 150\n&quot;, &quot;rmsprop_lr = 0.01 # learning rate\n&quot;, &quot;\n&quot;, &quot;xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot;\n&quot;, &quot;epsilon = 1e-7 # small constant to avoid division by zero\n&quot;, &quot;new_z = 0\n&quot;, &quot;dy_dx_current_rmsprop = 0\n&quot;, &quot;\n&quot;, &quot;dy_dx_new_rmsprop = torch.tensor([0.0, 0.0])&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def step_rmsprop(i):\n&quot;, &quot; global dy_dx_new_rmsprop, dy_dx_current_rmsprop, xys, rmsprop_lr, new_z, ax0, ax1\n&quot;, &quot; if i == 0:\n&quot;, &quot; # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot; xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot;\n&quot;, &quot; dy_dx_current_rmsprop = xys.grad\n&quot;, &quot; \n&quot;, &quot; cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n&quot;, &quot; dy_dx_new_rmsprop = 0.9*dy_dx_new_rmsprop + (1 - 0.9)*torch.pow(dy_dx_current_rmsprop,2)\n&quot;, &quot; xys = (xys - rmsprop_lr * (dy_dx_current_rmsprop/(torch.sqrt(dy_dx_new_rmsprop) + epsilon))).clone().detach().requires_grad_(True)\n&quot;, &quot; \n&quot;, &quot; # gradient descent with momentum\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n&quot;, &quot; dy_dx_current_rmsprop = xys.grad\n&quot;, &quot; \n&quot;, &quot; xys_plot = xys.detach().numpy()\n&quot;, &quot; ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='b', s=20, zorder=3)\n&quot;, &quot; a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n&quot;, &quot; [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n&quot;, &quot; lw=2, arrowstyle=\&quot;-|&gt;\&quot;, color=\&quot;k\&quot;)\n&quot;, &quot; ax0.add_artist(a)\n&quot;, &quot; \n&quot;, &quot; ax1.scatter(xys_plot[0], xys_plot[1], marker='*', c='b')&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;anim_rmsprop = animation.FuncAnimation(fig, step_rmsprop, frames=epochs, interval=(1/fps)*1000, repeat=False)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;# HTML(anim_rmsprop.to_html5_video())\n&quot;, &quot;anim_rmsprop.save('rmsprop.mp4', writer=writer)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Adam\n&quot;, &quot;***\n&quot;, &quot;### Gradient calculation\n&quot;, &quot;***\n&quot;, &quot;${\\partial (Error)}_{momentum} = \n&quot;, &quot;\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n&quot;, &quot;\\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} = \\beta_1 * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} + (1 - \\beta_1) * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial y}\n&quot;, &quot;\\end{vmatrix}$\n&quot;, &quot;\n&quot;, &quot;${\\partial (Error)}_{rmsprop} = \n&quot;, &quot;\\frac{\\partial (Error)}{\\partial (w_{x,y}^l)} = \n&quot;, &quot;\\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} = \\beta_2 * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error)}{\\partial y}\n&quot;, &quot;\\end{vmatrix} + (1 - \\beta_2) * \\begin{vmatrix}\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial x} \\\\\n&quot;, &quot;\\frac{\\partial (Error_{new})}{\\partial y}\n&quot;, &quot;\\end{vmatrix}^2$\n&quot;, &quot;\n&quot;, &quot;### Update equation\n&quot;, &quot;***\n&quot;, &quot;$w_{x,y}^l = w_{x,y}^l - lr \\times \\frac{\\partial (Error)_{momentum}}{\\sqrt{\\partial (Error)_{rmsprop}} + \\epsilon}$&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 7, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;epochs = 240\n&quot;, &quot;adam_lr = 0.01 # learning rate\n&quot;, &quot;\n&quot;, &quot;xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot;\n&quot;, &quot;epsilon = 1e-7 # small constant to avoid division by zero\n&quot;, &quot;new_z = 0\n&quot;, &quot;dy_dx_current_mom = 0\n&quot;, &quot;dy_dx_current_rmsprop = 0\n&quot;, &quot;\n&quot;, &quot;dy_dx_new = torch.tensor([0.0, 0.0])&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def step_adam(i):\n&quot;, &quot; global dy_dx_current_mom, dy_dx_current_rmsprop, dy_dx_new, xys, adam_lr, new_z, ax0, ax1\n&quot;, &quot; if i == 0:\n&quot;, &quot; # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4])\n&quot;, &quot; xys = torch.tensor([-0.5, -0.7], requires_grad=True)\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot;\n&quot;, &quot; dy_dx_new = xys.grad\n&quot;, &quot; \n&quot;, &quot; cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()]\n&quot;, &quot; \n&quot;, &quot; dy_dx_current_mom = 0.9*dy_dx_current_mom + (1 - 0.9)*dy_dx_new\n&quot;, &quot; dy_dx_current_rmsprop = 0.9*dy_dx_current_rmsprop + (1 - 0.9)*torch.pow(dy_dx_new,2)\n&quot;, &quot; xys = (xys - adam_lr * (dy_dx_current_mom/(torch.sqrt(dy_dx_current_rmsprop) + epsilon))).clone().detach().requires_grad_(True)\n&quot;, &quot; \n&quot;, &quot; # gradient descent with momentum\n&quot;, &quot; new_z = calc_z(xys[0], xys[1])\n&quot;, &quot; new_z.backward()\n&quot;, &quot; # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy)\n&quot;, &quot; dy_dx_new = xys.grad\n&quot;, &quot; \n&quot;, &quot; xys_plot = xys.detach().numpy()\n&quot;, &quot; ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker='s', c='c', s=20, zorder=3)\n&quot;, &quot; a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]],\n&quot;, &quot; [cache_pt[2], new_z.detach().numpy()], mutation_scale=5,\n&quot;, &quot; lw=2, arrowstyle=\&quot;-|&gt;\&quot;, color=\&quot;k\&quot;)\n&quot;, &quot; ax0.add_artist(a)\n&quot;, &quot; \n&quot;, &quot; ax1.scatter(xys_plot[0], xys_plot[1], marker='*', c='c')&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;anim_adam = animation.FuncAnimation(fig, step_adam, frames=epochs, interval=(1/fps)*1000, repeat=False)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: null, &quot;metadata&quot;: {}, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;# HTML(anim_adam.to_html5_video())\n&quot;, &quot;anim_adam.save('adam.mp4', writer=writer)&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;&quot; ] }, { &quot;cell_type&quot;: &quot;markdown&quot;, &quot;metadata&quot;: {}, &quot;source&quot;: [ &quot;## Results\n&quot;, &quot;\n&quot;, &quot;We see that all the algorithms find the minimas but take significatnly different paths. While Vanilla gradient descent and gradient descent with momentum find the minima faster compared to RMSprop and Adam here for the same learning rate, studies have proven Adam to be more stable and this ability allows to use higher learning rates as compared to the same learning rates used here.&quot; ] } ], &quot;metadata&quot;: { &quot;kernelspec&quot;: { &quot;display_name&quot;: &quot;Python 3&quot;, &quot;language&quot;: &quot;python&quot;, &quot;name&quot;: &quot;python3&quot; }, &quot;language_info&quot;: { &quot;codemirror_mode&quot;: { &quot;name&quot;: &quot;ipython&quot;, &quot;version&quot;: 3 }, &quot;file_extension&quot;: &quot;.py&quot;, &quot;mimetype&quot;: &quot;text/x-python&quot;, &quot;name&quot;: &quot;python&quot;, &quot;nbconvert_exporter&quot;: &quot;python&quot;, &quot;pygments_lexer&quot;: &quot;ipython3&quot;, &quot;version&quot;: &quot;3.8.3&quot; } }, &quot;nbformat&quot;: 4, &quot;nbformat_minor&quot;: 4 }</summary></entry><entry><title type="html">Google kickstart Mural coding question(2018 RoundH)</title><link href="https://logicatcore.github.io/scratchpad/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H.html" rel="alternate" type="text/html" title="Google kickstart Mural coding question(2018 RoundH)" /><published>2020-09-10T00:00:00-05:00</published><updated>2020-09-10T00:00:00-05:00</updated><id>https://logicatcore.github.io/scratchpad/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H</id><author><name></name></author><category term="python" /><category term="coding competition" /><summary type="html">Importance of reusing the results (Dynamic programming)</summary></entry><entry><title type="html">Topology constrained search</title><link href="https://logicatcore.github.io/scratchpad/2020/08/31/Akka-Coding-Challenge.html" rel="alternate" type="text/html" title="Topology constrained search" /><published>2020-08-31T00:00:00-05:00</published><updated>2020-08-31T00:00:00-05:00</updated><id>https://logicatcore.github.io/scratchpad/2020/08/31/Akka-Coding-Challenge</id><author><name></name></author><category term="MATLAB" /><category term="programming competition" /><summary type="html">Objective</summary></entry><entry><title type="html">Finding the coordinates of non-zero pixels in sparse images/matrices</title><link href="https://logicatcore.github.io/scratchpad/2020/08/13/sparse-image-coordinates.html" rel="alternate" type="text/html" title="Finding the coordinates of non-zero pixels in sparse images/matrices" /><published>2020-08-13T00:00:00-05:00</published><updated>2020-08-13T00:00:00-05:00</updated><id>https://logicatcore.github.io/scratchpad/2020/08/13/sparse-image-coordinates</id><author><name></name></author><category term="python" /><category term="algorithms" /><summary type="html">Sparse images/matrices are those in which the contained useful information is less compared to total space being occupied.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://logicatcore.github.io/scratchpad/images/sparse_coords/intro_sparse.png" /><media:content medium="image" url="https://logicatcore.github.io/scratchpad/images/sparse_coords/intro_sparse.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Fastpages Notebook Blog Post</title><link href="https://logicatcore.github.io/scratchpad/jupyter/2020/02/20/test.html" rel="alternate" type="text/html" title="Fastpages Notebook Blog Post" /><published>2020-02-20T00:00:00-06:00</published><updated>2020-02-20T00:00:00-06:00</updated><id>https://logicatcore.github.io/scratchpad/jupyter/2020/02/20/test</id><author><name></name></author><category term="jupyter" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://logicatcore.github.io/scratchpad/images/chart-preview.png" /><media:content medium="image" url="https://logicatcore.github.io/scratchpad/images/chart-preview.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>