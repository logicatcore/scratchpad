{
  
    
        "post0": {
            "title": "K-Means from scratch visualised with 1D, 2D and 3D data",
            "content": "Concept . K-Means is a unsupervised clustering algorithm which is analogous to supervised classification algorithms. Due to the name, K-Means algorithm is often confused with supervised KNN(K Nearest Neighbhours) algorithm which is used for both classification and regression problems. . As the name suggests, K-Means algorithm comprises of &quot;K&quot; &quot;Means&quot; which correspond to the number of clusters that the algorithm tries to find in the unlabeled data. The working of the algorithm though quite simple the challenge lies in scaling the algorithm for large datasets and picking an appropriate measure for distance. . Before we go into any more details, let us have a look at the steps that comprise the K-Means algorithm- . Inputs to the algorithm- Data to cluster | Number of clusters to identify | Convergence criteria i.e. when to stop Usually a tolerance value is given which is used to observe when the &quot;Mean&quot; position doesn&#39;t change any more | Other option is the maximum number of iterations to carry out | | Function to use as a Measure - usually cartesian distance is used but when data is text or any other abstract data, special measures have to be choosen | | Based on the data space i.e. the bounds (minimums and maximums) and the number of clusters to identify, that many random Mean points in the same space as the data are generated | Distance of all the data samples/points in the data space, say $ mathbb{R}^n$, with resepect to K number of Means are calculated (i.e. if there are 10 samples and 2 clusters to find, the number of distance measures calculated are 20(1x10 + 1x10)) | Based on the distances each data sample is associated to their closest Mean | Based on the associations made in step 4 the Mean values are updated by calculating the mean of all the values associated with one particular Mean. This is done for K number of Means | The steps 3 to 5 are then repeated either until the algorithm exceeds the maximum number of allowed iterations or until the values of K Means have settled down i.e. the change after update is less than the tolerance value specified in the input | Next, we turn to the implementation and coding part of the algorithm and discuss the results. . Code start . Handling the imports . %matplotlib widget import time import IPython import numpy as np import pandas as pd import numpy.linalg as LA import matplotlib.pyplot as plt from matplotlib import animation from mpl_toolkits.mplot3d import Axes3D from IPython.display import Video plt.rcParams[&#39;figure.figsize&#39;] = [8.0, 8.0] plt.rcParams[&#39;figure.dpi&#39;] = 100 . K-Means class to hold data, methods to compute the distances, update the means, and plot the progress and results . class kmeans: def __init__(self, dimensions, sample_size, clusters, tolerance, max_iters): &quot;&quot;&quot; Use the initialisation parameters as attributes of the object and create a matplotlib figure canvas on which each iteration progress can be drawn :param dimesnions: Dimension of data :param sample_size: Not necessary for real data, here used for generating sample data :param clusters: Number of clusters to identify in the data, control the number of Means :param tolerance: The tolearance value :param max_iters: Maximum iterations to execute &quot;&quot;&quot; self.dimensions = dimensions self.sample_size = sample_size self.clusters = clusters self.tolerance = tolerance self.max_iters = max_iters self.colors = [&#39;y&#39;, &#39;r&#39;, &#39;b&#39;, &#39;m&#39;, &#39;k&#39;, &#39;c&#39;, &#39;b&#39;, &#39;m&#39;, &#39;k&#39;, &#39;c&#39;] if self.dimensions == 1: self.fig, self.ax = plt.subplots(1,1) self.sample_pts = np.array([[]]) self.ax.grid(True) elif self.dimensions == 2: self.fig, self.ax = plt.subplots(1,1) self.sample_pts = np.array([[], []]) self.ax.grid(True) elif self.dimensions == 3: self.fig = plt.figure(1, figsize=(8, 6)) self.sample_pts = np.array([[], [], []]) self.ax = Axes3D(self.fig, rect=(0.0, 0.0, .95, 1.0), elev=48, azim=134) def kmeans_init(self): &quot;&quot;&quot; Generate sample data and draw the initial state of the data and display the initial position of the Means &quot;&quot;&quot; ################################################################################################################################## # Creating clusters using normal distribution and random variance and mean # every cluster will have equal number of points ################################################################################################################################## for i in range(0, self.clusters): np.random.seed(int((-i) ** 2)) tmp = np.random.randn(1, (self.sample_size // self.clusters) * self.dimensions) * np.random.randint(1, 10) + np.random.randint(-100, 100) self.sample_pts = np.hstack((self.sample_pts, tmp.reshape(self.dimensions, self.sample_size // self.clusters))) np.random.seed(22) self.previous_means = np.random.randn(self.clusters, self.dimensions) * np.random.randint(1, 12) # Randomly selected means i.e., cluster centers # print(f&#39;Starting means are: {self.previous_means}&#39;) self.new_means = np.zeros((self.clusters, self.dimensions)) # To store the new means after every iteration ################################################################################################################################## # plot initial means and all data samples to see the distribution ################################################################################################################################## if self.dimensions == 1: self.ax.scatter(self.previous_means[:, 0], np.zeros((self.clusters, 1)), marker=&#39;o&#39;, c=&#39;r&#39;, label=&#39;Initial Means&#39;) self.ax.scatter(self.sample_pts[0, :], np.zeros((1, self.sample_size)), marker=&#39;*&#39;) # Plotting all the points to see the clusters elif self.dimensions == 2: self.ax.scatter(self.previous_means[:, 0], self.previous_means[:, 1], marker=&#39;o&#39;, c=&#39;r&#39;, label=&#39;Initial Means&#39;) self.ax.scatter(self.sample_pts[0, :], self.sample_pts[1, :], marker=&#39;*&#39;) # Plotting all the points to see the clusters elif self.dimensions == 3: self.ax.scatter(self.previous_means[:, 0], self.previous_means[:, 1], self.previous_means[:, 2], marker=&#39;o&#39;, c=&#39;r&#39;, label=&#39;Initial Means&#39;, depthshade=False) self.ax.scatter(self.sample_pts[0, :], self.sample_pts[1, :], self.sample_pts[2, :], marker=&#39;*&#39;) # Plotting all the points to see the clusters self.ax.legend(loc=&#39;upper right&#39;) ################################################################################################################################## # Loop till convergence ################################################################################################################################## def kmeans_iter(self, iteration_count): &quot;&quot;&quot; Iteration part of the algorithm which iterates until the tolerance criteria is met while limiting the maximum number of iterations to preven infinite loops when the algorithm cannot associate a Mean value with a cluster &quot;&quot;&quot; if (abs(self.previous_means - self.new_means) &gt; self.tolerance).any() and (iteration_count &lt; self.max_iters) and (iteration_count != 0): print(f&#39;Iteration number {iteration_count}&#39;) if iteration_count != 1: self.previous_means = self.new_means.copy() dist = pd.DataFrame() ################################################################################################################################## # Compute distances of all points with respect to each mean ################################################################################################################################## for i in range(0, self.clusters): # distance_to_mean_1_iter_1 naming used dist[&#39;dtm_&#39; + str(i + 1) + f&#39;_iter_{iteration_count}&#39;] = LA.norm( self.sample_pts - self.previous_means[i, :].T.reshape(self.dimensions, 1), axis=0) # Assign a data sample to the mean it is nearest to by extracting the digit in the name of the index i.e., column # name where the minimum value is found # dtm_{1}_iter_1 dist[&#39;assign_to_mean&#39;] = dist.idxmin(axis=1).str[4] ################################################################################################################################## # compute the new means based on the classes assigned ################################################################################################################################## for i in range(0, self.clusters): indices = dist.assign_to_mean[dist.assign_to_mean == str(i + 1)] if self.dimensions &gt; 1: if len(indices.index) != 0: self.new_means[i, :] = np.mean(self.sample_pts[:, indices.index], axis=1) else: # Re-initialise a mean if it is not associated with any data sample self.new_means[i, :] = np.random.randn(1, self.dimensions) * 100 else: if len(indices.index) != 0: self.new_means[i, 0] = np.mean(self.sample_pts[0, indices.index]) else: # Re-initialise a mean if it is not associated with any data sample self.new_means[i, 0] = np.random.randn(1, self.dimensions) * 100 # print(f&#39;New means are:{self.new_means}&#39;) ################################################################################################################################## # Plot the movement of the means ################################################################################################################################## if self.dimensions == 1: for i in range(0, self.clusters): self.ax.plot([self.previous_means[i, 0], self.new_means[i, 0]], [0, 0], label=&#39;mean movement&#39; if iteration_count == 1 else &quot;&quot;, c=self.colors[i]) self.ax.scatter(self.new_means[i, 0], 0, marker=&#39;o&#39;, c=&#39;g&#39;, label=&#39;new Means&#39; if i == 0 and iteration_count == 1 else &quot;&quot;) elif self.dimensions == 2: for i in range(0, self.clusters): self.ax.plot([self.previous_means[i, 0], self.new_means[i, 0]], [self.previous_means[i, 1], self.new_means[i, 1]], label=&#39;mean movement&#39; if iteration_count == 1 else &quot;&quot;, c=self.colors[i]) self.ax.scatter(self.new_means[i, 0], self.new_means[i, 1], marker=&#39;o&#39;, c=&#39;g&#39;, label=&#39;new Means&#39; if i == 0 and iteration_count == 1 else &quot;&quot;) elif self.dimensions == 3: for i in range(0, self.clusters): self.ax.plot([self.previous_means[i, 0], self.new_means[i, 0]], [self.previous_means[i, 1], self.new_means[i, 1]], [self.previous_means[i, 2], self.new_means[i, 2]], label=&#39;mean movement&#39; if iteration_count == 1 else &quot;&quot;, c=self.colors[i]) self.ax.scatter(self.new_means[i, 0], self.new_means[i, 1], self.new_means[i, 2], marker=&#39;o&#39;, c=&#39;g&#39;, label=&#39;new Means&#39; if i == 0 and iteration_count == 1 else &quot;&quot;) self.ax.legend(loc=&#39;upper right&#39;) # iteration_count += 1 # self.fig.canvas.draw() ################################################################################################################################## # Plot the clustering results upon convergence ################################################################################################################################## if (abs(self.previous_means - self.new_means) &lt; self.tolerance).all(): cluster_pts = [] division = self.sample_size // self.clusters if self.dimensions == 1: for i in range(0, self.clusters): indices = dist.assign_to_mean[dist.assign_to_mean == str(i + 1)] cluster_pts.append(len(indices.index)) self.ax.scatter(self.sample_pts[0, indices.index], np.zeros((1, cluster_pts[i])), marker=&#39;*&#39;, label=f&#39;predicted cluster {i + 1}&#39;) self.ax.scatter(self.sample_pts[0, i * division:(i + 1) * division - 1], np.zeros((1, division - 1)), marker=&#39;o&#39;, facecolors=&#39;none&#39;, edgecolors=self.colors[i], s=200, linewidths=2, label=f&#39;real cluster {i + 1}&#39;) elif self.dimensions == 2: for i in range(0, self.clusters): indices = dist.assign_to_mean[dist.assign_to_mean == str(i + 1)] cluster_pts.append(len(indices.index)) self.ax.scatter(self.sample_pts[0, indices.index], self.sample_pts[1, indices.index], marker=&#39;*&#39;, label=f&#39;predicted cluster {i + 1}&#39;) self.ax.scatter(self.sample_pts[0, i * division:(i + 1) * division - 1], self.sample_pts[1, i * division:(i + 1) * division - 1], marker=&#39;o&#39;, facecolors=&#39;none&#39;, edgecolors=self.colors[i], s=200, linewidths=2, label=f&#39;real cluster {i + 1}&#39;) elif self.dimensions == 3: for i in range(0, self.clusters): indices = dist.assign_to_mean[dist.assign_to_mean == str(i + 1)] cluster_pts.append(len(indices.index)) self.ax.scatter(self.sample_pts[0, indices.index], self.sample_pts[1, indices.index], self.sample_pts[2, indices.index], marker=&#39;*&#39;, label=f&#39;predicted cluster {i + 1}&#39;) self.ax.scatter(self.sample_pts[0, i * division:(i + 1) * division - 1], self.sample_pts[1, i * division:(i + 1) * division - 1], self.sample_pts[2, i * division:(i + 1) * division - 1], marker=&#39;o&#39;, label=f&#39;real cluster {i + 1}&#39;, s=40) # facecolors=&#39;none&#39;, edgecolors=self.colors[i], s=200, linewidths=2) ################################################################################################################################## # set title with the clustering results and show legend ################################################################################################################################## if self.dimensions &lt; 3: self.ax.set_title(&#39;Number of points in each cluster are: &#39; + str(cluster_pts)) self.ax.legend(loc=&#39;upper right&#39;) else: self.ax.text2D(0.05, 0.95, &#39;Number of points in each cluster are: &#39; + str(cluster_pts), transform=self.ax.transAxes) self.ax.legend(loc=&#39;upper right&#39;) . Animation parameters . fps = 0.5 Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=fps, metadata=dict(artist=&#39;Sai&#39;), bitrate=1800) . 1D example of K-Means in action . max_iterations = 5 kmeans1d = kmeans(dimensions=1, sample_size=100, clusters=2, tolerance=1e-8, max_iters=max_iterations) animation.FuncAnimation(kmeans1d.fig, kmeans1d.kmeans_iter, init_func=kmeans1d.kmeans_init ,frames=max_iterations, interval=(1/fps)*1000, repeat=False).save(&#39;../images/kmeans/kmeans_1D.mp4&#39;, writer=writer); . In the video/animation below initially the data points in 1D space belonging to two clusters along with the initial random generated means are shown. After which the movement of means is shown and once the mean values are converged the results are shown. Since the data is being manually generated here, there is a possibility of verify how accurate the results are. As a last step for the verification, the predicted/identified clusters along with the supposed real clusters details are shown. . In this particular case the K-Means algorithm clusters accurately with 50 1D data samples of each of the two clusters. . Video(&quot;../images/kmeans/kmeans_1D.mp4&quot;, embed=True) . Your browser does not support the video tag. 2D example of K-Means in action . max_iterations = 15 kmeansnn2d = kmeans(dimensions=2, sample_size=600, clusters=4, tolerance=1e-8, max_iters=max_iterations) animation.FuncAnimation(kmeans2d.fig, kmeans2d.kmeans_iter, init_func=kmeans2d.kmeans_init ,frames=max_iterations, interval=(1/fps)*1000, repeat=False).save(&#39;../images/kmeans/kmeans_2D.mp4&#39;, writer=writer); . Like in previous case, here instead of 1D data samples 2D data samples are being used to cluster with the objective of identifying 4 clusters in the data. The movement of one of the Mean value if haphazard/chaotic because when a Mean cannot associate with any of the data sample, the position of the Mean is again randomly initialised, this randomness ensures that exactly 4 clusters will be found even if the starting position of the Mean values are not ideal. . In this case we also get to see that the clustering is not 100% accurate, as the final clustering results result in 150, 150, 152 and 148 data samples being associated to each cluster which ideally should have been a even 150 value for all the 4 clusters. . Video(&quot;../images/kmeans/kmeans_2D.mp4&quot;, embed=True) . Your browser does not support the video tag. 3D example of K-Means in action . max_iterations = 8 kmeans3d = kmeans(dimensions=3, sample_size=300, clusters=3, tolerance=1e-8, max_iters=max_iterations) animation.FuncAnimation(kmeans3d.fig, kmeans3d.kmeans_iter, init_func=kmeans3d.kmeans_init ,frames=max_iterations, interval=(1/fps)*1000, repeat=False).save(&#39;../images/kmeans/kmeans_3D.mp4&#39;, writer=writer); . Like in previous cases, here instead of 1D or 2D data samples 3D data samples are being used to cluster with the objective of identifying 3 clusters in the data. . Video(&quot;../images/kmeans/kmeans_3D.mp4&quot;, embed=True) . Your browser does not support the video tag. Remarks . In this post/jupyter notebook the distance measure used is the &quot;cartesian distance&quot;, but other distance measures like &quot;cosine&quot; distance can also be used. In this post/jupyter notebook the data used is also randomly generated numerical data, but when data is textual the implementation details vary. .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/2021/01/15/K-Means.html",
            "relUrl": "/machine%20learning/jupyter/2021/01/15/K-Means.html",
            "date": " • Jan 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Vanilla Regression Decision Tree example",
            "content": "Decision Tree . What is a decision tree? . Well, a decision tree is the most simplistic way of reasoning to find order in chaos or a system. Although very rudimentry in nature, decision trees and its varients(bagging methods and boosting methods) are capable of mapping i.e. finding the order in data to a very high degree of accuracy. Decision trees are mainly binary tress comprising of if-else statements that determine the split lines, planes or hyperplanes which breaks down the data space in $ mathbb{R}^n$ dimensions into rectangles, cuboids or hyper cuboids. . What are we trying to do here? . This post/jupyter notebook is meant for visualising how the decision line for data in $ mathbb{R}^1$ space. . Approximated algorithm of Regression Decision Tree implemented here . Divide the dimension in $ mathbb{R}^1$ space into equally spaced intervals | At each interval compute the mean of all the points to the left and to the right of the interval | Compute the Mean Squared Error(MSE) of all the points to the left of the interval with respect to the mean obtained in step 2 and do the same respectively for the points to the right of the interval. | Add up the MSE&#39;s and keep track of them | Repeat steps 2 to 4 at all the intervals | Determine the interval where the MSE was minimum, this interval point is the optimal point which classifies the input data into two groups | Code start . Imports are handled and graph style presets are set here . import numpy as np import matplotlib.pyplot as plt from matplotlib import animation from IPython.display import Video %matplotlib widget plt.style.use(&#39;seaborn&#39;) . Helper functions to calculate the mean and mean squared error of data points . def mean(x, y, split_at): ml, mr = np.mean(y[x &lt; split_at]), np.mean(y[x &gt; split_at]) return ml, mr def mse(x, y, split_at, ml, mr): msel, mser = np.mean(np.square(ml - y[x &lt; split_at])), np.mean(np.square(mr - y[x &gt; split_at])) return msel, mser . Initialise random data. The data is generated using two normal distributions with their respective means and variances. Equal data points from each normal distribution are sampled. Here, a method is also declared to perform the repetitive steps 2 to 4 as per the above algorithm. . class Draw: def __init__(self, samples): &quot;&quot;&quot; Get the number of data samples to generate and prepare the figure &quot;&quot;&quot; self.fig = plt.figure() self.ax = self.fig.add_subplot(111) self.ax.set_xlabel(&#39;x&#39;) self.ax.set_ylabel(r&#39;$f(x)$&#39;) self.samples = samples # Data along the dimension of independent variable # 1st data cluster- mean=0, variance=0.1, count=samples/2 # 2nd data cluster- mean=0.7, variance=0.1, count=samples/2 self.x = np.hstack((np.random.normal(0, 0.1, (1, samples//2)), np.random.normal(0.7, 0.1, (1, samples//2)))) self.xmin = np.min(self.x) self.xmax = np.max(self.x) # Divide the search space along the independent variable x into equal intervals self.linspace = np.linspace(np.min(self.x), np.max(self.x), samples) # Data along the dimension of dependent variable(ex--&gt; Temperature over the day or year) # 1st data cluster- mean=0, variance=0.2, count=samples/2 # 2nd data cluster- mean=0.5, variance=0.2, count=samples/2 self.f = np.hstack((np.random.normal(0, 0.2, (1, samples//2)), np.random.normal(0.5, 0.2, (1, samples//2)))) self.fmax = np.max(self.f) self.mses = [] self.ax.scatter(self.x, self.f, marker= &#39;.&#39;) def step_tree(self, i): splitting_at = self.linspace[i+1] # self.ax.clear() for artist in plt.gca().lines + plt.gca().collections: artist.remove() left_mean, right_mean = mean(self.x, self.f, splitting_at) left_mse, right_mse = mse(self.x, self.f, splitting_at, left_mean, right_mean) self.mses.append(left_mse+right_mse) self.ax.scatter(self.x[self.x &lt; splitting_at], self.f[self.x &lt; splitting_at], c = &#39;c&#39;) self.ax.scatter(self.x[self.x &gt; splitting_at], self.f[self.x &gt; splitting_at], c = &#39;k&#39;) xllim, xrlim = self.ax.get_xlim() self.ax.axvline(splitting_at, 0, 1, c=&#39;r&#39;, linewidth=2) self.ax.axhline(left_mean, 0, (splitting_at-xllim)/(xrlim - xllim), c=&#39;c&#39;, linewidth=2, linestyle=&#39;-.&#39;, label=&#39;Left Mean&#39;) ymin = self.f.copy() ymax = ymin.copy() ymax[(self.x &lt; splitting_at) &amp; (self.f &lt; left_mean)] = left_mean ymin[(self.x &lt; splitting_at) &amp; (self.f &gt; left_mean)] = left_mean ymax[(self.x &gt; splitting_at) &amp; (self.f &lt; right_mean)] = right_mean ymin[(self.x &gt; splitting_at) &amp; (self.f &gt; right_mean)] = right_mean self.ax.vlines(self.x[(self.x &lt; splitting_at)], ymin[(self.x &lt; splitting_at)], ymax[(self.x &lt; splitting_at)], color=&#39;c&#39;, linestyle=&#39;--&#39;) self.ax.vlines(self.x[(self.x &gt; splitting_at)], ymin[(self.x &gt; splitting_at)], ymax[(self.x &gt; splitting_at)], color=&#39;k&#39;, linestyle=&#39;--&#39;) self.ax.axhline(right_mean, (splitting_at-xllim)/(xrlim - xllim), 1, c=&#39;k&#39;, linewidth=2, linestyle=&#39;-.&#39;, label=&#39;Right Mean&#39;) #print(len(self.linspace[:i+1]), len(self.mses)) self.ax.plot(self.linspace[:len(self.mses)], self.mses, c=&#39;r&#39;, label=&#39;MSE error&#39;) if i == self.samples-3: # self.ax.clear() for artist in plt.gca().lines + plt.gca().collections: artist.remove() self.ax.scatter(self.x[self.x &lt; self.linspace[np.argmin(self.mses)]], self.f[self.x &lt; self.linspace[np.argmin(self.mses)]], c = &#39;c&#39;, label=&#39;leaf-1 points&#39;) self.ax.scatter(self.x[self.x &gt; self.linspace[np.argmin(self.mses)]], self.f[self.x &gt; self.linspace[np.argmin(self.mses)]], c = &#39;k&#39;, label=&#39;leaf-2 points&#39;) self.ax.axhline(left_mean, 0, (self.linspace[np.argmin(self.mses)]-xllim)/(xrlim - xllim), c=&#39;c&#39;, linewidth=2, linestyle=&#39;-.&#39;, label=&#39;Left Mean&#39;) self.ax.axhline(right_mean, (self.linspace[np.argmin(self.mses)]-xllim)/(xrlim - xllim), 1, c=&#39;k&#39;, linewidth=2, linestyle=&#39;-.&#39;, label=&#39;Right Mean&#39;) self.ax.plot(self.linspace[:len(self.mses)], self.mses, c=&#39;r&#39;, label=&#39;MSE error&#39;) # self.ax.scatter(self.linspace[np.argmin(self.mses)], np.min(self.mses), c=&#39;g&#39;, marker=&#39;*&#39;, label=&#39;Minimum MSE&#39;) self.ax.axvline(self.linspace[np.argmin(self.mses)], c=&#39;b&#39;, label=&#39;Decision boundary at minimum MSE&#39;) self.ax.set_title(&#39;Decision boundary is at &#39;+str(self.linspace[np.argmin(self.mses)])) self.ax.legend(loc=&#39;upper left&#39;) . fps = 10 # frames per second samples = 50 # number of data samples assert samples % 2 == 0, &quot;Please specify an even number of data samples&quot; draw_obj = Draw(samples) Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=fps, metadata=dict(artist=&#39;Sai&#39;), bitrate=1800) animation.FuncAnimation(draw_obj.fig, draw_obj.step_tree, frames=samples-2, interval=(1/fps)*1000, repeat=False).save(&#39;decision_tree.mp4&#39;, writer=writer) . Video(&quot;../images/decision_tree/decision_tree.mp4&quot;, embed=True) . Your browser does not support the video tag. Results . As the algorithm progresses/steps through the equally spaced intervals we get to see how the means of left and right data points with respect to the interval position(red vertical line) varies and how this influence the MSE error. Finally we determine the interval where the mean was the minimum and hence choose this interval i.e. point along the independent variable x which divides the data into two sections. . The results of the entire operation is the final splitting boundary which determines wether it is either day or night, summer or winter and based on this independent variable x the decision tree enables us to take an educated guess about the possible temperature or any other possible value of f(x), which is nothing but the mean values of left and right leafs. .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/2021/01/11/Regression-Decision-Tree.html",
            "relUrl": "/machine%20learning/jupyter/2021/01/11/Regression-Decision-Tree.html",
            "date": " • Jan 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Graphviz Artifical Neural Networks visualisation",
            "content": "Graphviz scripts to create simple visualisations of neural networks . Simple feed forward neural networks . Often while working with neural networks in Deep Learning and Machine Learning fields it is often easy to picturise the network architecture into a concise diagram which conveys lot of useful information. In what follows are some scripts that are inspired from martisak dotnets github repo including a rudementry graphing script which generates a DOT lannguage script interpretable by graphviz. The script found on the github repo was later ported to graphviz and expanded to add more functionalities to the script. The added functionalities enable activation function annotations and recurrent neural networks representation. . Firstly the graphviz module is imported and some global variables are set . try: import graphviz as G except ImportError as e: print(&#39;ModuleNotFoundError: &quot;graphviz&quot; package not available, install it with &quot;pip install graphviz&quot;&#39;) # boolean variables to denote dense or sparse connections between layers DENSE = True SPARSE = False PENWIDTH = &#39;15&#39; FONT = &#39;Hilda 10&#39; . Now the network architecture details are defined based on which a neural network will be graphed. The details include the number of nodes/perceptrons present in each layer, the type of connections between two layers. Since the connections are in-between layers, the length of connections list has to be 1 less than the length of layers list. . layer_nodes = [6, 4, 4, 4] connections = [DENSE, DENSE, SPARSE] assert len(connections) == (len(layer_nodes) - 1), &#39;&quot;connections&quot; array should be 1 less than the #layers&#39; for i, type_of_connections in enumerate(connections): if type_of_connections == SPARSE: assert layer_nodes[i] == layer_nodes[i+1], &quot;If connection type is SPARSE then the number of nodes in the adjacent layers must be equal&quot; . A graph in graphviz mainly consists of three components, namely, nodes, edges, and the graph itself. Just like defining a class while programming before creating an object, overhere the graphviz library provides a generic Digraph class which is to be instantiated with our desired object presets. The following piece of code instantiates a directed graph with some nodes, edges, and graph attributes based on which the graph will be drawn. . dot = G.Digraph(comment=&#39;Neural Network&#39;, graph_attr={&#39;nodesep&#39;:&#39;0.04&#39;, &#39;ranksep&#39;:&#39;0.05&#39;, &#39;bgcolor&#39;:&#39;white&#39;, &#39;splines&#39;:&#39;line&#39;, &#39;rankdir&#39;:&#39;LR&#39;, &#39;fontname&#39;:FONT}, node_attr={&#39;fixedsize&#39;:&#39;true&#39;, &#39;label&#39;:&quot;&quot;, &#39;style&#39;:&#39;filled&#39;, &#39;color&#39;:&#39;none&#39;, &#39;fillcolor&#39;:&#39;gray&#39;, &#39;shape&#39;:&#39;circle&#39;, &#39;penwidth&#39;:PENWIDTH, &#39;width&#39;:&#39;0.4&#39;, &#39;height&#39;:&#39;0.4&#39;}, edge_attr={&#39;color&#39;:&#39;gray30&#39;, &#39;arrowsize&#39;:&#39;.4&#39;}) . Create nodes . for layer_no in range(len(layer_nodes)): with dot.subgraph(name=&#39;cluster_&#39;+str(layer_no)) as c: c.attr(color=&#39;transparent&#39;) # comment this if graph background is needed if layer_no == 0: # first layer c.attr(label=&#39;Input&#39;) elif layer_no == len(layer_nodes)-1: # last layer c.attr(label=&#39;Output&#39;) else: # layers in between c.attr(label=&#39;Hidden&#39;) for a in range(layer_nodes[layer_no]): if layer_no == 0: # or i == len(layers)-1: # first or last layer c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;&#39;, fillcolor=&#39;black&#39;)#, fontcolor=&#39;white&#39; if layer_no == len(layer_nodes)-1: c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;&#39;, fontcolor=&#39;white&#39;, fillcolor=&#39;black&#39;)#, fontcolor=&#39;white&#39; else: # unicode characters can be used to inside the nodes as follows # for a list of unicode characters refer this https://pythonforundergradengineers.com/unicode-characters-in-python.html c.node(&#39;l&#39;+str(layer_no)+str(a), &#39; u03C3&#39;, fontsize=&#39;12&#39;) # to place &quot;sigma&quot; inside the nodes of a layer # for normal textual representation like &#39;relu&#39; and &#39;tanh&#39;, the following approach can be taken # c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;relu&#39;, fontsize=&#39;12&#39;) . Create edges . for layer_no in range(len(layer_nodes)-1): for node_no in range(layer_nodes[layer_no]): if connections[layer_no] == DENSE: for b in range(layer_nodes[layer_no+1]): dot.edge(&#39;l&#39;+str(layer_no)+str(node_no), &#39;l&#39;+str(layer_no+1)+str(b),) elif connections[layer_no] == SPARSE: dot.edge(&#39;l&#39;+str(layer_no)+str(node_no), &#39;l&#39;+str(layer_no+1)+str(node_no)) . Render . dot . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_0 Input cluster_1 Hidden cluster_2 Hidden cluster_3 Output l00 σ l10 σ l00&#45;&gt;l10 l11 σ l00&#45;&gt;l11 l12 σ l00&#45;&gt;l12 l13 σ l00&#45;&gt;l13 l01 σ l01&#45;&gt;l10 l01&#45;&gt;l11 l01&#45;&gt;l12 l01&#45;&gt;l13 l02 σ l02&#45;&gt;l10 l02&#45;&gt;l11 l02&#45;&gt;l12 l02&#45;&gt;l13 l03 σ l03&#45;&gt;l10 l03&#45;&gt;l11 l03&#45;&gt;l12 l03&#45;&gt;l13 l04 σ l04&#45;&gt;l10 l04&#45;&gt;l11 l04&#45;&gt;l12 l04&#45;&gt;l13 l05 σ l05&#45;&gt;l10 l05&#45;&gt;l11 l05&#45;&gt;l12 l05&#45;&gt;l13 l20 σ l10&#45;&gt;l20 l21 σ l10&#45;&gt;l21 l22 σ l10&#45;&gt;l22 l23 σ l10&#45;&gt;l23 l11&#45;&gt;l20 l11&#45;&gt;l21 l11&#45;&gt;l22 l11&#45;&gt;l23 l12&#45;&gt;l20 l12&#45;&gt;l21 l12&#45;&gt;l22 l12&#45;&gt;l23 l13&#45;&gt;l20 l13&#45;&gt;l21 l13&#45;&gt;l22 l13&#45;&gt;l23 l30 l20&#45;&gt;l30 l31 l21&#45;&gt;l31 l32 l22&#45;&gt;l32 l33 l23&#45;&gt;l33 Save/Export . dot.format = &#39;JPEG&#39; # or PDF, SVG, JPEG, PNG, etc. . dot.render(&#39;./example_network&#39;) . &#39;./example_network.jpeg&#39; . Recurrent neural network . The previously used code can be modified and adapted easily for variuous architectures, for adding additional details, for customizing, and much more. The same code if now modified to represent recurrent neural networks instead. . layer_nodes = [6, 4, 4, 4] connections = [DENSE, DENSE, DENSE] # additional variable to denote which layers consist of recurrent units recurrent = [False, True, True, False] assert len(connections) == (len(layer_nodes) - 1), &#39;&quot;connections&quot; array should be 1 less than the #layers&#39; for i, type_of_connections in enumerate(connections): if type_of_connections == SPARSE: assert layer_nodes[i] == layer_nodes[i+1], &quot;If connection type is SPARSE then the number of nodes in the adjacent layers must be equal&quot; dot = G.Digraph(comment=&#39;Neural Network&#39;, graph_attr={&#39;nodesep&#39;:&#39;0.04&#39;, &#39;ranksep&#39;:&#39;0.05&#39;, &#39;bgcolor&#39;:&#39;white&#39;, &#39;splines&#39;:&#39;line&#39;, &#39;rankdir&#39;:&#39;LR&#39;, &#39;fontname&#39;:FONT}, node_attr={&#39;fixedsize&#39;:&#39;true&#39;, &#39;label&#39;:&quot;&quot;, &#39;style&#39;:&#39;filled&#39;, &#39;color&#39;:&#39;none&#39;, &#39;fillcolor&#39;:&#39;gray&#39;, &#39;shape&#39;:&#39;circle&#39;, &#39;penwidth&#39;:PENWIDTH, &#39;width&#39;:&#39;0.4&#39;, &#39;height&#39;:&#39;0.4&#39;}, edge_attr={&#39;color&#39;:&#39;gray30&#39;, &#39;arrowsize&#39;:&#39;.4&#39;}) for layer_no in range(len(layer_nodes)): with dot.subgraph(name=&#39;cluster_&#39;+str(layer_no)) as c: c.attr(color=&#39;transparent&#39;) # comment this if graph background is needed if layer_no == 0: # first layer c.attr(label=&#39;Input&#39;) elif layer_no == len(layer_nodes)-1: # last layer c.attr(label=&#39;Output&#39;) else: # layers in between c.attr(label=&#39;Hidden&#39;) for a in range(layer_nodes[layer_no]): if layer_no == 0: # or i == len(layers)-1: # first or last layer c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;&#39;, fillcolor=&#39;black&#39;)#, fontcolor=&#39;white&#39; if layer_no == len(layer_nodes)-1: c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;&#39;, fontcolor=&#39;white&#39;, fillcolor=&#39;black&#39;)#, fontcolor=&#39;white&#39; else: # unicode characters can be used to inside the nodes as follows # for a list of unicode characters refer this https://pythonforundergradengineers.com/unicode-characters-in-python.html # c.node(&#39;l&#39;+str(layer_no)+str(a), &#39; u03C3&#39;, fontsize=&#39;12&#39;) # to place &quot;sigma&quot; inside the nodes of a layer # for normal textual representation like &#39;relu&#39; and &#39;tanh&#39;, the following approach can be taken c.node(&#39;l&#39;+str(layer_no)+str(a), &#39;tanh&#39;, fontsize=&#39;12&#39;) for layer_no in range(len(layer_nodes)-1): for node_no in range(layer_nodes[layer_no]): if connections[layer_no] == DENSE: # to place recuurent units # change the label &#39;x10&#39; to denote the number of time steps into which the recurrent unit unrolls in time if recurrent[layer_no]: dot.edge(&#39;l&#39;+str(layer_no)+str(node_no), &#39;l&#39;+str(layer_no)+str(node_no), xlabel=&#39;x10&#39;, color=&#39;blue&#39;, fontcolor=&#39;blue&#39;) for b in range(layer_nodes[layer_no+1]): dot.edge(&#39;l&#39;+str(layer_no)+str(node_no), &#39;l&#39;+str(layer_no+1)+str(b),) elif connections[layer_no] == SPARSE: dot.edge(&#39;l&#39;+str(layer_no)+str(node_no), &#39;l&#39;+str(layer_no+1)+str(node_no)) . dot . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_0 Input cluster_1 Hidden cluster_2 Hidden cluster_3 Output l00 tanh l10 tanh l00&#45;&gt;l10 l11 tanh l00&#45;&gt;l11 l12 tanh l00&#45;&gt;l12 l13 tanh l00&#45;&gt;l13 l01 tanh l01&#45;&gt;l10 l01&#45;&gt;l11 l01&#45;&gt;l12 l01&#45;&gt;l13 l02 tanh l02&#45;&gt;l10 l02&#45;&gt;l11 l02&#45;&gt;l12 l02&#45;&gt;l13 l03 tanh l03&#45;&gt;l10 l03&#45;&gt;l11 l03&#45;&gt;l12 l03&#45;&gt;l13 l04 tanh l04&#45;&gt;l10 l04&#45;&gt;l11 l04&#45;&gt;l12 l04&#45;&gt;l13 l05 tanh l05&#45;&gt;l10 l05&#45;&gt;l11 l05&#45;&gt;l12 l05&#45;&gt;l13 l10&#45;&gt;l10 x10 l20 tanh l10&#45;&gt;l20 l21 tanh l10&#45;&gt;l21 l22 tanh l10&#45;&gt;l22 l23 tanh l10&#45;&gt;l23 l11&#45;&gt;l11 x10 l11&#45;&gt;l20 l11&#45;&gt;l21 l11&#45;&gt;l22 l11&#45;&gt;l23 l12&#45;&gt;l12 x10 l12&#45;&gt;l20 l12&#45;&gt;l21 l12&#45;&gt;l22 l12&#45;&gt;l23 l13&#45;&gt;l13 x10 l13&#45;&gt;l20 l13&#45;&gt;l21 l13&#45;&gt;l22 l13&#45;&gt;l23 l20&#45;&gt;l20 x10 l30 l20&#45;&gt;l30 l31 l20&#45;&gt;l31 l32 l20&#45;&gt;l32 l33 l20&#45;&gt;l33 l21&#45;&gt;l21 x10 l21&#45;&gt;l30 l21&#45;&gt;l31 l21&#45;&gt;l32 l21&#45;&gt;l33 l22&#45;&gt;l22 x10 l22&#45;&gt;l30 l22&#45;&gt;l31 l22&#45;&gt;l32 l22&#45;&gt;l33 l23&#45;&gt;l23 x10 l23&#45;&gt;l30 l23&#45;&gt;l31 l23&#45;&gt;l32 l23&#45;&gt;l33 Save/Export . dot.format = &#39;pdf&#39; # or PDF, SVG, JPEG, PNG, etc. . dot.render(&#39;./example_recurrent_network&#39;) . &#39;./example_recurrent_network.pdf&#39; . Some more additional scripts that are useful . 1.Unrolled representation of a single recurrent unit . TIMESTEPS = 6 TIME_OFFSET = 3 unrolled = G.Digraph(node_attr={&#39;shape&#39;:&#39;circle&#39;, &#39;fixedsize&#39;:&#39;true&#39;}, graph_attr={&#39;style&#39;:&#39;invis&#39;, &#39;rankdir&#39;:&#39;BT&#39;, &#39;color&#39;:&#39;transparent&#39;}) . for step in range(TIMESTEPS+2): if step == 0 or step == TIMESTEPS+1: with unrolled.subgraph(name=&#39;cluster_&#39;+str(i)) as c: c.node(&#39;a&#39;+str(step), &#39;&#39;, color=&#39;transparent&#39;) c.node(&#39;b&#39;+str(step), &#39;...&#39;, color=&#39;transparent&#39;) c.node(&#39;c&#39;+str(step), &#39;&#39;, color=&#39;transparent&#39;) c.edge(&#39;a&#39;+str(step), &#39;b&#39;+str(step), style=&#39;invis&#39;) c.edge(&#39;b&#39;+str(step), &#39;c&#39;+str(step), style=&#39;invis&#39;) else: with unrolled.subgraph(name=&#39;cluster_&#39;+str(i)) as c: c.node(&#39;a&#39;+str(step), &#39;&#39;, color=&#39;transparent&#39;) c.node(&#39;b&#39;+str(step), &#39;t&#39;+&#39;{:=+d}&#39;.format(TIME_OFFSET-step) if TIME_OFFSET-step else &#39;t&#39;) c.node(&#39;c&#39;+str(step), &#39;&#39;, color=&#39;transparent&#39;); c.edge(&#39;a&#39;+str(step), &#39;b&#39;+str(step)); c.edge(&#39;b&#39;+str(step), &#39;c&#39;+str(step)); . for step in range(1, TIMESTEPS+2): unrolled.edge(&#39;b&#39;+str(step-1), &#39;b&#39;+str(step), constraint=&#39;false&#39;, dir=&#39;back&#39;, color=&#39;blue&#39;) . unrolled . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster_2 a0 b0 ... c0 b1 t+2 b0&#45;&gt;b1 a1 a1&#45;&gt;b1 c1 b1&#45;&gt;c1 b2 t+1 b1&#45;&gt;b2 a2 a2&#45;&gt;b2 c2 b2&#45;&gt;c2 b3 t b2&#45;&gt;b3 a3 a3&#45;&gt;b3 c3 b3&#45;&gt;c3 b4 t&#45;1 b3&#45;&gt;b4 a4 a4&#45;&gt;b4 c4 b4&#45;&gt;c4 b5 t&#45;2 b4&#45;&gt;b5 a5 a5&#45;&gt;b5 c5 b5&#45;&gt;c5 b6 t&#45;3 b5&#45;&gt;b6 a6 a6&#45;&gt;b6 c6 b6&#45;&gt;c6 b7 ... b6&#45;&gt;b7 a7 c7 unrolled.render(&#39;./unrolled&#39;) . &#39;./unrolled.pdf&#39; . 2.Single recurrent unit . ru = G.Digraph(node_attr={&#39;shape&#39;:&#39;circle&#39;, &#39;fixedsize&#39;:&#39;true&#39;}, graph_attr={&#39;style&#39;:&#39;invis&#39;, &#39;rankdir&#39;:&#39;LR&#39;}) . ru.node(&#39;a&#39;, &#39;&#39;, color=&#39;transparent&#39;) ru.node(&#39;b&#39;, &#39;N&#39;) ru.node(&#39;c&#39;, &#39;&#39;, color=&#39;transparent&#39;) ru.edge(&#39;a&#39;, &#39;b&#39;) ru.edge(&#39;b&#39;, &#39;c&#39;) ru.edge(&#39;b&#39;, &#39;b&#39;, color=&#39;blue&#39;) . ru . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 a b N a&#45;&gt;b b&#45;&gt;b c b&#45;&gt;c ru.render(&#39;./rnn&#39;) . &#39;./rnn.pdf&#39; .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/graphviz/2020/12/26/Graphviz-Neural-Networks-visualisation.html",
            "relUrl": "/machine%20learning/jupyter/graphviz/2020/12/26/Graphviz-Neural-Networks-visualisation.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Input data representation as 2D array of 3D blocks",
            "content": ". Often while working with machine learning algorithms the developer has a good picture of how the input data looks like apart from knowing what the input data is. Also, most of the times the input data is usually represented or decribed with array terminology. Hence, this particular post is one such attempt to create simple 2D representations of 3D-blocks symbolising the arrays used for input. . Graphviz a highly versatile graphing library that creates graphs based on DOT language is used to create the 2D array representation of 3D blocks with annotation and color uniformity to create quick and concise graphs/pictures for good explanations of input data used in various machine learning/deep learning algorithms. . In what follows is a script to create the 2D array representation og 3D blocks mainly intented for time-series data. The script facilitates some features which include- . Starting at time instant 0 or -1 | counting backwards i.e. t-4 -&gt; t-3 -&gt; t-2 -&gt; t-1 -&gt; t-0 or counting forwards t-0 -&gt; t-1 -&gt; t-2 -&gt; t-3 -&gt; t-4 -&gt; t-5 | . Imports and global constants . import graphviz as G # to create the required graphs import random # to generate random hex codes for colors FORWARDS = True # to visualise array from left to right BACKWARDS = False # to visualise array from right to left . Properties of 2D representation of 3D array blocks . Main features/properties of the array visualisation needed are defined gere before actually creating the graph/picture. 1) Number of Rows: similar to rows in a matrix where each each row corresponds to one particular data type with data across different time instants arranged in columns . 2) Blocks: which corresponds to the number of time instants in each row (jagged arrays can also be graphed) . 3) Prefix: the annotation used to annotate each 3D block in the 2D array representation . ROW_NUMS = [1, 2] # Layer numbers corresponding to the number of rows of array data (must be contiguous) BLOCKS = [3, 3] # number of data fields in each row i.e., columns in each row diff = [x - ROW_NUMS[i] for i, x in enumerate(ROW_NUMS[1:])] assert diff == [1]*(len(ROW_NUMS) - 1), &#39;&quot;layer_num&quot; should contain contiguous numbers only&#39; assert len(ROW_NUMS) == len(BLOCKS), &quot;&#39;cells&#39; list and &#39;layer_num&#39; list should contain same number of entries&quot; direction = BACKWARDS # control the direction of countdown of timesteps INCLUDE_ZERO = True # for time series based data START_AT = 0 if INCLUDE_ZERO else 1 # names = [[&#39;Softmax nprobabilities&#39;, &#39;p1&#39;, &#39;p2&#39;, &#39;p3&#39;, &#39;p4&#39;, &#39;p5&#39;, &#39;p6&#39;, &#39;p7&#39;, &#39;p8&#39;, &#39;p9&#39;, &#39;p10&#39;],[&#39;&#39;, &#39; +&#39;, &#39; +&#39;, &#39; +&#39;, &#39; +&#39;, &#39; +&#39;, &#39; +&#39;],[&#39;GMM nprobabilities&#39;, &#39;p1&#39;, &#39;p2&#39;, &#39;p3&#39;, &#39;p4&#39;, &#39;p5&#39;, &#39;p6&#39;]] # the trick to adding symbols like the &quot;partial(dou)&quot; i.e. &#39;∂&#39; is to write these symbols in a markdown cell using the $ partial$ utilising the mathjax support and # copying the content after being rendered and paste in the code as a string wherever needed prefix = [&#39;∂(i)-&#39;, &#39;∂(v)-&#39;] . r = lambda: random.randint(0,255) # to generate random colors for each row # intantiate a directed graph with intial properties dot = G.Digraph(comment=&#39;Matrix&#39;, graph_attr={&#39;nodesep&#39;:&#39;0.02&#39;, &#39;ranksep&#39;:&#39;0.02&#39;, &#39;bgcolor&#39;:&#39;transparent&#39;}, node_attr={&#39;shape&#39;:&#39;box3d&#39;,&#39;fixedsize&#39;:&#39;true&#39;, &#39;width&#39;:&#39;1.1&#39;}) for row_no in ROW_NUMS: if row_no != 1: dot.edge(str(row_no-1)+str(START_AT), str(row_no)+str(START_AT), style=&#39;invis&#39;) # invisible edges to contrain layout with dot.subgraph() as sg: sg.attr(rank=&#39;same&#39;) color = &#39;#{:02x}{:02x}{:02x}&#39;.format(r(),r(),r()) for block_no in range(START_AT, BLOCKS[row_no-1]+START_AT): if direction: sg.node(str(row_no)+str(block_no), &#39;t-&#39;+str(block_no), style=&#39;filled&#39;, fillcolor=color) else: if START_AT == 0: sg.node(str(row_no)+str(block_no), prefix[row_no-1]+str(BLOCKS[row_no-1]-block_no-1), style=&#39;filled&#39;, fillcolor=color) else: sg.node(str(row_no)+str(block_no), prefix[row_no-1]+str(BLOCKS[row_no-1]-block_no-1), style=&#39;filled&#39;, fillcolor=color) . Render . dot . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 10 ∂(i)&#45;2 20 ∂(v)&#45;2 11 ∂(i)&#45;1 12 ∂(i)&#45;0 21 ∂(v)&#45;1 22 ∂(v)&#45;0 Save/Export . . dot.render(&#39;./lstm_input&#39;) . &#39;./lstm_input.pdf&#39; . Additional script to just show the breakdown of train-test data of the dataset being used . import random r = lambda: random.randint(0,255) # to generate random colors for each row . folders = G.Digraph(node_attr={&#39;style&#39;:&#39;filled&#39;}, graph_attr={&#39;style&#39;:&#39;invis&#39;, &#39;rankdir&#39;:&#39;LR&#39;},edge_attr={&#39;color&#39;:&#39;black&#39;, &#39;arrowsize&#39;:&#39;.2&#39;}) . color = &#39;#{:02x}{:02x}{:02x}&#39;.format(r(),r(),r()) with folders.subgraph(name=&#39;cluster0&#39;) as f: f.node(&#39;root&#39;, &#39;Dataset n x2000&#39;, shape=&#39;folder&#39;, fillcolor=color) . color = &#39;#{:02x}{:02x}{:02x}&#39;.format(r(),r(),r()) with folders.subgraph(name=&#39;cluster1&#39;) as f: f.node(&#39;train&#39;, &#39;Train n 1800&#39;, shape=&#39;note&#39;, fillcolor=color) f.node(&#39;test&#39;, &#39;Test n x200&#39;, shape=&#39;note&#39;, fillcolor=color) . folders.edge(&#39;root&#39;, &#39;train&#39;) folders.edge(&#39;root&#39;, &#39;test&#39;) . folders . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 cluster0 cluster1 root Dataset x2000 train Train 1800 root&#45;&gt;train test Test x200 root&#45;&gt;test folders.render(&#39;./dataset&#39;) . &#39;./dataset.pdf&#39; .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/graphviz/2020/12/26/Array-Visualiser.html",
            "relUrl": "/machine%20learning/jupyter/graphviz/2020/12/26/Array-Visualiser.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Optimization algorithms using pytorch",
            "content": "Optimization algorithms using pytorch . Optimization algorithms play a central role in the learning process of most of the machine learning and deep learning algorithms. Here are some of the well known algorithms- . Vanilla Gradient descent | Gradient descent with Momentum | RMSprop | Adam | While all the 4 above listed algorithms differ in their own way and have certain advantages and disadvantages. They share certain similarities with the simple graddient descent algorithm. In this blog post we will go through these 4 algorithms and see how they function on minimizing the loss or finding the minima of a random error function with multiple minimas and maximas implemented using pytorch. . Error function with multiple minimas and maximas . Error Function $= f(x,y) = 3 times e^{(-(y + 1)^2 - x^2)} times (x - 1)^2 - frac{e^{(-(x + 1)^2 - y^2)}}{3} + e^{(-x^2 - y^2)} times (10x^3 - 2x + 10y^5)$ . . Note: In this blog post, I will not be going into the theory of all the algorithms used rather just concentrate on the implementation and the results . For theoretical reference please refer to d2lai chapter on optimization algorithms . Import all the necessary python modules . %matplotlib widget import torch import IPython import numpy as np from IPython import display from IPython.display import HTML, Video import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import animation from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d import proj3d from matplotlib.patches import FancyArrowPatch # mpl.rcParams[&#39;savefig.dpi&#39;] = 300 plt.style.use(&#39;seaborn&#39;) . To draw arrows to monitor the progress of optimization . class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs) self._verts3d = xs, ys, zs def draw(self, renderer): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) FancyArrowPatch.draw(self, renderer) . Error or loss calculation based on 2 independent parameters . def calc_z(xx, yy)-&gt; torch.tensor: &quot;&quot;&quot; Returns the loss at a certain point &quot;&quot;&quot; return 3 * torch.exp(-(yy + 1) ** 2 - xx ** 2) * (xx - 1) ** 2 - torch.exp(-(xx + 1) ** 2 - yy ** 2) / 3 + torch.exp( -xx ** 2 - yy ** 2) * (10 * xx ** 3 - 2 * xx + 10 * yy ** 5) . fps = 10 # frames per second - to save the progress in optimization as a video Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=fps, metadata=dict(artist=&#39;Me&#39;), bitrate=1800) . Initialise the plot with the error function terrain . x = torch.linspace(-3, 3, 600) y = torch.linspace(-3, 3, 600) xgrid, ygrid = torch.meshgrid(x, y) zgrid = calc_z(xgrid, ygrid) fig = plt.figure(figsize=(14,6)) ax0 = fig.add_subplot(121, projection=&#39;3d&#39;) ax0.set_xlabel(&#39;$x$&#39;) ax0.set_ylabel(&#39;$y$&#39;) ax0.set_zlabel(&#39;Random Loss function: &#39; + &#39;$f(x, y)$&#39;) ax0.axis(&#39;auto&#39;) cs = ax0.plot_surface(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), cmap=&#39;viridis&#39;, alpha=0.6) fig.colorbar(cs) ax1 = fig.add_subplot(122) qcs = ax1.contour(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), 20, cmap=&#39;viridis&#39;) fig.colorbar(qcs) . &lt;matplotlib.colorbar.Colorbar at 0x7f7dccd822b0&gt; . . Vanilla Gradient Descent . Gradient descent algorithm which is an iterative optimization algorithm can be described as loop which is executed repeatedly until certain convergence criteria has been met. Gradient descent can be explained using the following equation. . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix}$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)}{ partial (w_{x,y}^l)}$ . epochs = 20 lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) new_z = 0 dy_dx_current = 0 . def step_gd(i): global dy_dx_current, xys, lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] xys = (xys - lr * dy_dx_current).clone().detach().requires_grad_(True) # vanilla gradient descent new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;r&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;r&#39;) . anim_gd = animation.FuncAnimation(fig, step_gd, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_gd.save(&#39;gd.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/gd.mp4&quot;, embed=True) . Your browser does not support the video tag. Gradient descent with momentum . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)}{ partial (w_{x,y}^l)}$ . epochs = 60 lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) new_z = 0 dy_dx_current_gdm = 0 dy_dx_new_gdm = torch.tensor([0.0, 0.0]) . def step_gdm(i): global dy_dx_new_gdm, dy_dx_current_gdm, xys, lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current_gdm = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_new_gdm = 0.9*dy_dx_new_gdm + (1 - 0.9)*dy_dx_current_gdm xys = (xys - lr * dy_dx_new_gdm).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current_gdm = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;g&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;g&#39;) . anim_gdm = animation.FuncAnimation(fig, step_gdm, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_gdm.save(&#39;momentum.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/momentum.mp4&quot;, embed=True) . Your browser does not support the video tag. RMSprop . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}^2$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ frac{ partial (Error_{new})}{ partial (w_{x,y}^l)}}{ sqrt{ frac{ partial (Error)}{ partial (w_{x,y}^l)} + epsilon}}$ . epochs = 150 rmsprop_lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) epsilon = 1e-7 # small constant to avoid division by zero new_z = 0 dy_dx_current_rmsprop = 0 dy_dx_new_rmsprop = torch.tensor([0.0, 0.0]) . def step_rmsprop(i): global dy_dx_new_rmsprop, dy_dx_current_rmsprop, xys, rmsprop_lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current_rmsprop = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_new_rmsprop = 0.9*dy_dx_new_rmsprop + (1 - 0.9)*torch.pow(dy_dx_current_rmsprop,2) xys = (xys - rmsprop_lr * (dy_dx_current_rmsprop/(torch.sqrt(dy_dx_new_rmsprop) + epsilon))).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current_rmsprop = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;b&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;b&#39;) . anim_rmsprop = animation.FuncAnimation(fig, step_rmsprop, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_rmsprop.save(&#39;rmsprop.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/rmsprop.mp4&quot;, embed=True) . Your browser does not support the video tag. Adam . Gradient calculation . ${ partial (Error)}_{momentum} = frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta_1 * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta_1) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}$ . ${ partial (Error)}_{rmsprop} = frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta_2 * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta_2) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}^2$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)_{momentum}}{ sqrt{ partial (Error)_{rmsprop}} + epsilon}$ . epochs = 240 adam_lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) epsilon = 1e-7 # small constant to avoid division by zero new_z = 0 dy_dx_current_mom = 0 dy_dx_current_rmsprop = 0 dy_dx_new = torch.tensor([0.0, 0.0]) . def step_adam(i): global dy_dx_current_mom, dy_dx_current_rmsprop, dy_dx_new, xys, adam_lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_new = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_current_mom = 0.9*dy_dx_current_mom + (1 - 0.9)*dy_dx_new dy_dx_current_rmsprop = 0.9*dy_dx_current_rmsprop + (1 - 0.9)*torch.pow(dy_dx_new,2) xys = (xys - adam_lr * (dy_dx_current_mom/(torch.sqrt(dy_dx_current_rmsprop) + epsilon))).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_new = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;c&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;c&#39;) . anim_adam = animation.FuncAnimation(fig, step_adam, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_adam.save(&#39;adam.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/adam.mp4&quot;, embed=True) . Your browser does not support the video tag. Results . We see that all the algorithms find the minimas but take significatnly different paths. While Vanilla gradient descent and gradient descent with momentum find the minima faster compared to RMSprop and Adam here for the same learning rate, studies have proven Adam to be more stable and this ability allows to use higher learning rates as compared to the same learning rates used here. .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/2020/10/28/Machine-Learning-Optimization-Algorithms.html",
            "relUrl": "/machine%20learning/jupyter/2020/10/28/Machine-Learning-Optimization-Algorithms.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Google kickstart Mural coding question(2018 RoundH)",
            "content": "Importance of reusing the results (Dynamic programming) . Problem statement: Summary . . There are N sections on a wall and you can paint only one of the sections in a day, the next day you are allowed to paint the section which is adjacent to a painted one only. Also, everyday one section gets destroyed and the section which gets destroyed are always at the ends (adjacent to unpainted ones) . In the above figure you will see one of the many possibilities. . P# -&gt; Painted section on day # | D# -&gt; Destroyed section on day # | . Problem description . Thanh wants to paint a wonderful mural on a wall that is N sections long. Each section of the wall has a beauty score, which indicates how beautiful it will look if it is painted. Unfortunately, the wall is starting to crumble due to a recent flood, so he will need to work fast! At the beginning of each day, Thanh will paint one of the sections of the wall. On the first day, he is free to paint any section he likes. On each subsequent day, he must paint a new section that is next to a section he has already painted, since he does not want to split up the mural. At the end of each day, one section of the wall will be destroyed. It is always a section of wall that is adjacent to only one other section and is unpainted (Thanh is using a waterproof paint, so painted sections can’t be destroyed). The total beauty of Thanh’s mural will be equal to the sum of the beauty scores of the sections he has painted. Thanh would like to guarantee that, no matter how the wall is destroyed, he can still achieve a total beauty of at least B. What’s the maximum value of B for which he can make this guarantee? . Input . The first line of the input gives the number of test cases, T. T test cases follow. Each test case starts with a line containing an integer N. Then, another line follows containing a string of N digits from 0 to 9. The i-th digit represents the beauty score of the i-th section of the wall. . Output . For each test case, output one line containing Case #x: y, where x is the test case number (starting from 1) and y is the maximum beauty score that Thanh can guarantee that he can achieve, as described above. . Limits . 1 ≤ T ≤ 100. Time limit: 20 seconds per test set. Memory limit: 1 GB. . Small dataset (Test set 1 - Visible) . 2 ≤ N ≤ 100. . Large dataset (Test set 2 - Hidden) . For exactly 1 case, N = 5 × 106; for the other T - 1 cases, 2 ≤ N ≤ 100. Sample . Meaning Input Output . Cases | 4 | Case #1: 6 | . #Sections | 4 | Case #2: 14 | . Beauty scores | 1332 | Case #3: 7 | . #Sections | 4 | Case #4: 31 | . Beauty scores | 9583 |   | . #Sections | 3 |   | . Beauty scores | 616 |   | . #Sections | 10 |   | . Beauty scores | 1029384756 |   | . In the first sample case, Thanh can get a total beauty of 6, no matter how the wall is destroyed. On the first day, he can paint either section of wall with beauty score 3. At the end of the day, either the 1st section or the 4th section will be destroyed, but it does not matter which one. On the second day, he can paint the other section with beauty score 3. . | In the second sample case, Thanh can get a total beauty of 14, by painting the leftmost section of wall (with beauty score 9). The only section of wall that can be destroyed is the rightmost one, since the leftmost one is painted. On the second day, he can paint the second leftmost section with beauty score 5. Then the last unpainted section of wall on the right is destroyed. Note that on the second day, Thanh cannot choose to paint the third section of wall (with beauty score 8), since it is not adjacent to any other painted sections. . | In the third sample case, Thanh can get a total beauty of 7. He begins by painting the section in the middle (with beauty score 1). Whichever section is destroyed at the end of the day, he can paint the remaining wall at the start of the second day. . | . Solution . . Looking at the problem and the sample solutions, it is clear that the painted sections are always contiguous(next to each other as chain link) and the length of the painted sections is ceil(N/2). . The solution to the problem is quite simple in it’s own worth but the challenge is to come up with a efficient solution to solve for large number of sections!! The solution to the problem is max of the rolling sum of roll length ceil(N/2) . If you are familiar with pandas library, the solution is a one line of code if we have all the beauty scores of the wall sections in a pandas Series- . import pandas as pd beauty_scores = pd.read_csv(&#39;input/path/to/file&#39;, delimiter=&#39; &#39;) result = pd.beauty_scores.rolling(math.ceil(N/2)).sum().max() . Read the data . . wall_sections = [] beauty_scores = [] with open(&#39;../inputs/mural_2018_H.txt&#39;) as file: cases = int(file.readline().rstrip()) for case in range(cases): wall_sections.append(int(file.readline().rstrip())) beauty_scores.append(list(map(int, list(file.readline().rstrip())))) . Solve the test case one by one . . We first calculate the roll length and then calculate the roll sums of the beauty scores . def solve_a(sections, bs): if sections % 2 == 0: roll = sections // 2 roll_sums = [sum(bs[i:i+roll]) for i in range(sections - roll + 1)] else: roll = sections // 2 + 1 roll_sums = [sum(bs[i:i+roll+1]) for i in range(sections - roll)] return max(roll_sums) for case in range(cases): result = solve(wall_sections[case], beauty_scores[case]) print(&quot;Case #{}: {}&quot;.format(case + 1, result)) . Let us have a look at the number of operations involved- . roll_windows = N - roll_length | summations = (roll_windows) * roll_length | comparisons = roll_windows | . i.e., O(roll_windows + summations + comparisons) Note: The scales are in log . Improved solution, making use of previous results . Based on the operations breakdown we have seen just now, I see that a substantial number of summations can be avoided by utilizing the concept that every successive roll window overlaps the previous roll window except one element/beauty score To save on computations, we just have to add the beauty score of the new section and subtract which we do not want anymore. . Let us have a look at the number of operations involved- . roll_windows = N - roll_length | summations = (roll_windows) * 2 | comparisons = roll_windows | . i.e., O(roll_windows + summations + comparisons) . Based on the graphs it is clear that we see an improvement from O(10^(2log(N))) to O(10^log(N)) . def solve_b(sections, bs): if sections % 2 == 0: roll = sections // 2 tmp = sum(bs[:roll]) max_value = tmp for i in range(1, sections - roll + 1): tmp = tmp + bs[i+roll-1] - bs[i-1] if max_value &lt; tmp: max_value = tmp return max_value else: roll = sections // 2 + 1 tmp = sum(bs[:roll]) max_value = tmp for i in range(1, sections - roll + 1): tmp = tmp + bs[i + roll - 1] - bs[i - 1] if max_value &lt; tmp: max_value = tmp return max_value . Time comparisons . . . Slow(time in seconds) Fast(time in seconds) Improvement factor (times x) Wall sections . 7.91e-05 | 6.48e-05 | 1.22x | 4 | . 4.24e-05 | 3.95e-05 | 1.07x | 4 | . 4.36e-05 | 4.31e-05 | 1.01x | 3 | . 4.38e-05 | 9.08e-05 | 0.48x | 10 | . 0.00121 | 0.00146 | 0.83x | 500 | . 0.0784 | 0.0158 | 4.94x | 5000 | . 6.80 | 0.15 | 44.55x | 50000 | . 712.75 | 1.49 | 478.12x | 500000 | . Try it out . . If you feel like working out or if you have a much simpler and faster approach to solving this problem, I would like to see and learn!! . Here is the link to Test input and the test results if you want to cross check . Case #1: 6 Case #2: 14 Case #3: 7 Case #4: 31 Case #5: 1012 Case #6: 10129 Case #7: 100501 Case #8: 1001276 .",
            "url": "https://logicatcore.github.io/scratchpad/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H.html",
            "relUrl": "/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Topology constrained search",
            "content": "Topology constrained search . . Objective . . . To find the 2000th such number in the spiral arrangement of hexagons as seen above, which can divide the product of 6 neighbouring numbers perfectly i.e, the number in the central hexagon should be a factor of the product of the adjacent 6 numbers. . (20 * 37 * 19 * 2 * 9 * 21) / 8 = 664335 (20 * 37 * 19 * 2 * 9 * 21) % 8 = 0 . Logic behind the solution . . As there seems to be no pattern among the numbers distribution around any given hexagon which can be leveraged to find the neighbouring numbers of every hexagon. We resort to reproduce the hexagonal arrangement of numbers as per the problem statement in order to actually determine the neighbouring 6 numbers of any number we are interested in. . . | The next important step is to identify the 6 neighbouring numbers of all the numbers based on the euclidean distance of nearest 6 numbers . . | Next, we start to check if the center number is a factor of the product of the 6 neighbouring numbers. Trying to actually multiply and then divide will eventually bring us to the point where a double or float64 can preceisely store the value of multiplication and hence we need to solve this problem in a smart way. My approach to solving this was based on Prime factorisation . First, we find the prime factors of the center number and the surronding numbers | If all the prime factors of the center are present in the prime factors of all the 6 neighbouring numbers. Then the center number is a factor of the 6 other numbers | | Finally, stop the program when the 2000th number meeting our criteria is found . . | .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/31/Akka-Coding-Challenge.html",
            "relUrl": "/2020/08/31/Akka-Coding-Challenge.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Shortest route Dijkstra algorithm implementation",
            "content": "This area is not a drop target. . (Click the drag the nodes to interact with the graph) . Click to animate . Dijikstra’s algorithm . Dijkstra’s algorithm (or Dijkstra’s Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. . This particular coding challenge deals with a single source and single destination. The algorithm states to visit the child planets in the order of least cost and keep updating the path when a route with lower cost is found. . Example: . . Start: at node 1 . Iter: 1 . To vist: [2, 3, 4] in order (children of node 1) | Parent -&gt; [childs] | 2 -&gt; [3, 5] | 3 -&gt; [4, 5, 6] | 4 -&gt; [6] | . iter: 2 . To vist: [[3, 5], [4, 6, 5], [6]] in order (children of unvisited nodes only to avoid repetition) . | Cost of reaching 3 via 2 is 4+1 which is less than 1 -&gt; 3, so update the cost and path to reach 3 . | Cost of reaching 5 via 2 is 4+7 and this is first time we are visiting the node, so store the cost and path to reach 5 . | Cost of reaching 4 via 3 is 4+1+2 and this is less than 1 -&gt; 4, so update the path and cost to reach 4 . | Cost of reaching 6 via 3 is 4+1+4 and this is first time we are visiting the node, so store the cost and path to reach 6 . | Cost of reaching 5 via 3 is now 4+1+5 which is less than 1 -&gt; 3 -&gt; 5 because we have updated the path to reach 3 already and cost of paths 1 -&gt; 2 -&gt; 3 -&gt; 5(new) &lt; 1 -&gt; 2 -&gt; 5(existing path known), so update the cost and path to reach 5 . | Cost of reaching 6 via 4 is 4+1+2+5 and this is more than 1 -&gt; 2 -&gt; 3 -&gt; 6, so do not update the cost and path to reach 6 . | . iter:3 . To visit:[[7], [5, 7]] in order (children of unvisited nodes only to avoid repetition) | Cost of reaching 7 via 5 is 4+1+5+6 and this is first time we are visiting the node, so store the cost and path to reach 7 | Cost of reaching 5 via 6 is 4+1+4+1 and this is same as 1 -&gt; 2 -&gt; 3 -&gt; 5, so do not update the cost and path to reach 5 | Cost of reaching 7 via 6 is 4+1+4+8 and this is more than 1 -&gt; 2 -&gt; 3 -&gt; 5 -&gt; 7, so do not update the cost and path to reach 7 | . Task description . Find the only route from planet Erde to b3-r7-r4nd7 i.e, node 18 to node 246 among 1000 planets and 1500 routes possible in a bidirectional graph/map which allows moving from one planet to another with every route being associated with a cost value. . Working of code . -&gt; Works based on Dijkstra’s algorithm of single source and single destination . -&gt; Planets are visited based on least cost basis. The sub planets are recursively visited after one complete sweep again in least cost basis. . -&gt; Relaxation, current cost, current node, actual path of every planet is kept track in a matrix . -&gt; Execution stops when we reach our destination i.e, planet 246 or the planet named b3-r7-r4nd7 . Result . Path to the destination planet is: [ 18 810 595 132 519 71 432 246] Cost to reach the destination planet is: 2.995687895999458 . {&quot;source&quot;:18,&quot;target&quot;:810,&quot;cost&quot;:0.04060214221510905} {&quot;source&quot;:595,&quot;target&quot;:810,&quot;cost&quot;:0.1628038931266662} {&quot;source&quot;:132,&quot;target&quot;:595,&quot;cost&quot;:0.6331384762650787} {&quot;source&quot;:132,&quot;target&quot;:519,&quot;cost&quot;:0.6333618615566976} {&quot;source&quot;:71,&quot;target&quot;:519,&quot;cost&quot;:0.7625760415706333} {&quot;source&quot;:71,&quot;target&quot;:432,&quot;cost&quot;:0.6742157893614655} {&quot;source&quot;:246,&quot;target&quot;:432,&quot;cost&quot;:0.08898969190380779} . JSON data . Summary . In total there are 1000 Nodes and 1500 bidirectional paths . Every node has the following properties. . Original provided json . &quot;nodes&quot;: [{&quot;label&quot;:&quot;node_0&quot;}] &quot;edges&quot;: [{&quot;source&quot;:343,&quot;target&quot;:801,&quot;cost&quot;:0.8117216039041273}] . Modified json to visualise with sigma . &quot;nodes&quot;: [{&quot;color&quot;: &quot;#000000&quot;, &quot;label&quot;: &quot;0&quot;, &quot;y&quot;: -2.3181617858307746, &quot;x&quot;: 1.2953878183376322, &quot;id&quot;: &quot;0&quot;, &quot;size&quot;: 0.02775471971630339} &quot;edges&quot;: [{&quot;source&quot;:343,&quot;target&quot;:801,&quot;cost&quot;:0.8117216039041273}] . Data types conversion table between JSON &lt;-&gt; Python . | ++-+ | | JSON | Python | | +===============+===================+ | | object | dict | | ++-+ | | array | list | | ++-+ | | string | unicode | | ++-+ | | number (int) | int, long | | ++-+ | | number (real) | float | | ++-+ | | true | True | | ++-+ | | false | False | | ++-+ | +-++ | | Python | JSON | | +===================+===============+ | | dict | object | | +-++ | | list, tuple | array | | +-++ | | str, unicode | string | | +-++ | | int, long, float | number | | +-++ | | True | true | | +-++ | | False | false | | +-++ | | None | null | | +-++ . Address to website . -&gt; https://www.get-in-it.de/magazin/events/challenges/review-coding-in-the-galaxy .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/15/Finding-shortest-route-using-Dijkstra-algorithm.html",
            "relUrl": "/2020/08/15/Finding-shortest-route-using-Dijkstra-algorithm.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Finding the coordinates of non-zero pixels in sparse images/matrices",
            "content": "Sparse images/matrices are those in which the contained useful information is less compared to total space being occupied. . To begin with, we will look at an example image/matrix, the output we need and the main take aways from this post. . . Task: To determine the (row, column) values of all the non zero pixels in a matrix or an image. Typical example would be text in a image. If you are familiar with the well known MNIST Handwritten digits dataset, that could be another good example of a sparse image . . The threshold need not be zero and can be any arbitrary value of interest Example images that we will be working with . . . . . We will be going through two approaches in this short tutorial and also see how the methods compare in execution time . Method 1: The traditional and the first approach that comes to mind through for loops . Method 2: We can leverage the broadcasting properties of numpy and find a work around to reach the same result . Libraries used . . matplotlib | time | numpy | . Pre Prep . . First we need to create a sparse image to work with . import time import numpy as np import matplotlib.pyplot as plt # Image height and width dimensions rows = 100 columns = 100 img = np.random.rand(rows, columns) # Drawing number from uniform distribution [0,1] to avoid negative values img = img * 255 img[img &lt; 220] = 0 # this is optional and can be skipped to handle any threshold other than &#39;non zero value&#39; &amp; &#39;zero&#39; img = img.astype(np.uint8) print(f&quot;sparsity: {len(img[img != 0]) * 100 / np.cumproduct(img.shape)[-1]} %&quot;) . Method 1 . . x_coords = np.array([]) # To store column values y_coords = np.array([]) # To store row values start = time.time() for r in range(rows): for c in range(columns): if img[r][c] != 0: x_coords = np.concatenate((x_coords, np.array([c]))) y_coords = np.concatenate((y_coords, np.array([r]))) x_coords = x_coords.reshape(-1, 1) y_coords = y_coords.reshape(-1, 1) coords = np.hstack((y_coords, x_coords)) print(&quot;Finding non zero pixels coordinates with for loops took: &quot;, time.time() - start, &quot; seconds&quot;) . Method 2 . . First we create a template to go with our image dimension and make a boolean mask which we use to find the non zero pixel coordinates . coordinates_grid = np.ones((2, rows, columns), dtype=np.int16) coordinates_grid[0] = coordinates_grid[0] * np.array([range(rows)]).T coordinates_grid[1] = coordinates_grid[1] * np.array([range(rows)]) start = time.time() mask = img != 0 non_zero_coords = np.hstack((coordinates_grid[0][mask].reshape(-1, 1), coordinates_grid[1][mask].reshape(-1, 1))) print(&quot;Finding non zero pixels coordinates using broadcasting took: &quot;, time.time() - start, &quot; seconds&quot;) # print(&quot;Coordinates of the non zero pixels are: n&quot;, non_zero_coords) plt.imshow(img) plt.show() . Results . . sai@sai:~/****/scripts$ python coordinates.py 10 10 information: 13.0 % Finding non zero pixels coordinates with for loops took: 0.0003330707550048828 seconds Finding non zero pixels coordinates using broadcasting took: 3.4809112548828125e-05 seconds sai@sai:~/****/scripts$ python coordinates.py 100 100 information: 13.43 % Finding non zero pixels coordinates with for loops took: 0.024660587310791016 seconds Finding non zero pixels coordinates using broadcasting took: 0.00010442733764648438 seconds sai@sai:~/****/scripts$ python coordinates.py 1000 1000 information: 13.7047 % Finding non zero pixels coordinates with for loops took: 8.874347448348999 seconds Finding non zero pixels coordinates using broadcasting took: 0.007306575775146484 seconds . Conclusions . . It is clear that the execution times differ significantly and the benefits of vectorization becomes more dominant as the input data grows. .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/13/sparse-image-coordinates.html",
            "relUrl": "/2020/08/13/sparse-image-coordinates.html",
            "date": " • Aug 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Intro . My name is Sai Sharath Kakubal, I am from India and am currently pursuing my Masters course at Technische Hochschule Ingolstadt, Germany in International Automotive Engineering with a specialisation in Vehicle Electronics. . Interests and skills . Having pursued Mechanical Engineering in India and having been part of designing and building a Formula Student Race car during my Bachelor’s in a student team and having gotten opportunites to work as an Intern in a couple of companies has allowed me to hone my skills and build upon them progressively. My interests and skills are spread across a wide spectrum, and I especially like good presentation and like to learn new tools to complete any task in an efficient and an elegant way. . I am also passionate about Machine Learning and A.I fields and keep myself upto date with the latest advancements in these fields. . Programming languages that I have worked with . Python | C++ | Matlab | Java | C | .",
          "url": "https://logicatcore.github.io/scratchpad/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://logicatcore.github.io/scratchpad/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}