{
  
    
        "post0": {
            "title": "Optimization algorithms using pytorch",
            "content": "Optimization algorithms using pytorch . Optimization algorithms play a central role in the learning process of most of the machine learning and deep learning algorithms. Here are some of the well known algorithms- . Vanilla Gradient descent | Gradient descent with Momentum | RMSprop | Adam | While all the 4 above listed algorithms differ in their own way and have certain advantages and disadvantages. They share certain similarities with the simple graddient descent algorithm. In this blog post we will go through these 4 algorithms and see how they function on minimizing the loss or finding the minima of a random error function with multiple minimas and maximas implemented using pytorch. . Error function with multiple minimas and maximas . Error Function $= f(x,y) = 3 times e^{(-(y + 1)^2 - x^2)} times (x - 1)^2 - frac{e^{(-(x + 1)^2 - y^2)}}{3} + e^{(-x^2 - y^2)} times (10x^3 - 2x + 10y^5)$ . . Note: In this blog post, I will not be going into the theory of all the algorithms used rather just concentrate on the implementation and the results . For theoretical reference please refer to d2lai chapter on optimization algorithms . Import all the necessary python modules . %matplotlib widget import torch import IPython import numpy as np from IPython import display from IPython.display import HTML, Video import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib import animation from mpl_toolkits.mplot3d import Axes3D from mpl_toolkits.mplot3d import proj3d from matplotlib.patches import FancyArrowPatch # mpl.rcParams[&#39;savefig.dpi&#39;] = 300 plt.style.use(&#39;seaborn&#39;) . To draw arrows to monitor the progress of optimization . class Arrow3D(FancyArrowPatch): def __init__(self, xs, ys, zs, *args, **kwargs): FancyArrowPatch.__init__(self, (0, 0), (0, 0), *args, **kwargs) self._verts3d = xs, ys, zs def draw(self, renderer): xs3d, ys3d, zs3d = self._verts3d xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) FancyArrowPatch.draw(self, renderer) . Error or loss calculation based on 2 independent parameters . def calc_z(xx, yy)-&gt; torch.tensor: &quot;&quot;&quot; Returns the loss at a certain point &quot;&quot;&quot; return 3 * torch.exp(-(yy + 1) ** 2 - xx ** 2) * (xx - 1) ** 2 - torch.exp(-(xx + 1) ** 2 - yy ** 2) / 3 + torch.exp( -xx ** 2 - yy ** 2) * (10 * xx ** 3 - 2 * xx + 10 * yy ** 5) . fps = 10 # frames per second - to save the progress in optimization as a video Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=fps, metadata=dict(artist=&#39;Me&#39;), bitrate=1800) . Initialise the plot with the error function terrain . x = torch.linspace(-3, 3, 600) y = torch.linspace(-3, 3, 600) xgrid, ygrid = torch.meshgrid(x, y) zgrid = calc_z(xgrid, ygrid) fig = plt.figure(figsize=(14,6)) ax0 = fig.add_subplot(121, projection=&#39;3d&#39;) ax0.set_xlabel(&#39;$x$&#39;) ax0.set_ylabel(&#39;$y$&#39;) ax0.set_zlabel(&#39;Random Loss function: &#39; + &#39;$f(x, y)$&#39;) ax0.axis(&#39;auto&#39;) cs = ax0.plot_surface(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), cmap=&#39;viridis&#39;, alpha=0.6) fig.colorbar(cs) ax1 = fig.add_subplot(122) qcs = ax1.contour(xgrid.numpy(), ygrid.numpy(), zgrid.numpy(), 20, cmap=&#39;viridis&#39;) fig.colorbar(qcs) . &lt;matplotlib.colorbar.Colorbar at 0x7f7dccd822b0&gt; . . Vanilla Gradient Descent . Gradient descent algorithm which is an iterative optimization algorithm can be described as loop which is executed repeatedly until certain convergence criteria has been met. Gradient descent can be explained using the following equation. . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix}$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)}{ partial (w_{x,y}^l)}$ . epochs = 20 lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) new_z = 0 dy_dx_current = 0 . def step_gd(i): global dy_dx_current, xys, lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] xys = (xys - lr * dy_dx_current).clone().detach().requires_grad_(True) # vanilla gradient descent new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;r&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;r&#39;) . anim_gd = animation.FuncAnimation(fig, step_gd, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_gd.save(&#39;gd.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/gd.mp4&quot;, embed=True) . Your browser does not support the video tag. Gradient descent with momentum . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)}{ partial (w_{x,y}^l)}$ . epochs = 60 lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) new_z = 0 dy_dx_current_gdm = 0 dy_dx_new_gdm = torch.tensor([0.0, 0.0]) . def step_gdm(i): global dy_dx_new_gdm, dy_dx_current_gdm, xys, lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current_gdm = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_new_gdm = 0.9*dy_dx_new_gdm + (1 - 0.9)*dy_dx_current_gdm xys = (xys - lr * dy_dx_new_gdm).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current_gdm = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;g&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;g&#39;) . anim_gdm = animation.FuncAnimation(fig, step_gdm, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_gdm.save(&#39;momentum.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/momentum.mp4&quot;, embed=True) . Your browser does not support the video tag. RMSprop . Gradient calculation . $ frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}^2$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ frac{ partial (Error_{new})}{ partial (w_{x,y}^l)}}{ sqrt{ frac{ partial (Error)}{ partial (w_{x,y}^l)} + epsilon}}$ . epochs = 150 rmsprop_lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) epsilon = 1e-7 # small constant to avoid division by zero new_z = 0 dy_dx_current_rmsprop = 0 dy_dx_new_rmsprop = torch.tensor([0.0, 0.0]) . def step_rmsprop(i): global dy_dx_new_rmsprop, dy_dx_current_rmsprop, xys, rmsprop_lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_current_rmsprop = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_new_rmsprop = 0.9*dy_dx_new_rmsprop + (1 - 0.9)*torch.pow(dy_dx_current_rmsprop,2) xys = (xys - rmsprop_lr * (dy_dx_current_rmsprop/(torch.sqrt(dy_dx_new_rmsprop) + epsilon))).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_current_rmsprop = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;b&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;b&#39;) . anim_rmsprop = animation.FuncAnimation(fig, step_rmsprop, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_rmsprop.save(&#39;rmsprop.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/rmsprop.mp4&quot;, embed=True) . Your browser does not support the video tag. Adam . Gradient calculation . ${ partial (Error)}_{momentum} = frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta_1 * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta_1) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}$ . ${ partial (Error)}_{rmsprop} = frac{ partial (Error)}{ partial (w_{x,y}^l)} = begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} = beta_2 * begin{vmatrix} frac{ partial (Error)}{ partial x} frac{ partial (Error)}{ partial y} end{vmatrix} + (1 - beta_2) * begin{vmatrix} frac{ partial (Error_{new})}{ partial x} frac{ partial (Error_{new})}{ partial y} end{vmatrix}^2$ . Update equation . $w_{x,y}^l = w_{x,y}^l - lr times frac{ partial (Error)_{momentum}}{ sqrt{ partial (Error)_{rmsprop}} + epsilon}$ . epochs = 240 adam_lr = 0.01 # learning rate xys = torch.tensor([-0.5, -0.7], requires_grad=True) # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) epsilon = 1e-7 # small constant to avoid division by zero new_z = 0 dy_dx_current_mom = 0 dy_dx_current_rmsprop = 0 dy_dx_new = torch.tensor([0.0, 0.0]) . def step_adam(i): global dy_dx_current_mom, dy_dx_current_rmsprop, dy_dx_new, xys, adam_lr, new_z, ax0, ax1 if i == 0: # initialise starting point of search for minima, another possible starting position np.array([0.1, 1.4]) xys = torch.tensor([-0.5, -0.7], requires_grad=True) new_z = calc_z(xys[0], xys[1]) new_z.backward() dy_dx_new = xys.grad cache_pt = [xys[0].detach().numpy(), xys[1].detach().numpy(), new_z.detach().numpy()] dy_dx_current_mom = 0.9*dy_dx_current_mom + (1 - 0.9)*dy_dx_new dy_dx_current_rmsprop = 0.9*dy_dx_current_rmsprop + (1 - 0.9)*torch.pow(dy_dx_new,2) xys = (xys - adam_lr * (dy_dx_current_mom/(torch.sqrt(dy_dx_current_rmsprop) + epsilon))).clone().detach().requires_grad_(True) # gradient descent with momentum new_z = calc_z(xys[0], xys[1]) new_z.backward() # store the new gradient with respect to x and y i.e., (d(error))/ (dx), (d(error))/ (dy) dy_dx_new = xys.grad xys_plot = xys.detach().numpy() ax0.scatter(xys_plot[0], xys_plot[1], new_z.detach().numpy(), marker=&#39;s&#39;, c=&#39;c&#39;, s=20, zorder=3) a = Arrow3D([cache_pt[0], xys_plot[0]], [cache_pt[1], xys_plot[1]], [cache_pt[2], new_z.detach().numpy()], mutation_scale=5, lw=2, arrowstyle=&quot;-|&gt;&quot;, color=&quot;k&quot;) ax0.add_artist(a) ax1.scatter(xys_plot[0], xys_plot[1], marker=&#39;*&#39;, c=&#39;c&#39;) . anim_adam = animation.FuncAnimation(fig, step_adam, frames=epochs, interval=(1/fps)*1000, repeat=False) . anim_adam.save(&#39;adam.mp4&#39;, writer=writer) . Video(&quot;../images/optimization_algos/adam.mp4&quot;, embed=True) . Your browser does not support the video tag. Results . We see that all the algorithms find the minimas but take significatnly different paths. While Vanilla gradient descent and gradient descent with momentum find the minima faster compared to RMSprop and Adam here for the same learning rate, studies have proven Adam to be more stable and this ability allows to use higher learning rates as compared to the same learning rates used here. .",
            "url": "https://logicatcore.github.io/scratchpad/machine%20learning/jupyter/2020/10/28/Machine-Learning-Optimization-Algorithms.html",
            "relUrl": "/machine%20learning/jupyter/2020/10/28/Machine-Learning-Optimization-Algorithms.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Google kickstart Mural coding question(2018 RoundH)",
            "content": "Importance of reusing the results (Dynamic programming) . Problem statement: Summary . . There are N sections on a wall and you can paint only one of the sections in a day, the next day you are allowed to paint the section which is adjacent to a painted one only. Also, everyday one section gets destroyed and the section which gets destroyed are always at the ends (adjacent to unpainted ones) . In the above figure you will see one of the many possibilities. . P# -&gt; Painted section on day # | D# -&gt; Destroyed section on day # | . Problem description . Thanh wants to paint a wonderful mural on a wall that is N sections long. Each section of the wall has a beauty score, which indicates how beautiful it will look if it is painted. Unfortunately, the wall is starting to crumble due to a recent flood, so he will need to work fast! At the beginning of each day, Thanh will paint one of the sections of the wall. On the first day, he is free to paint any section he likes. On each subsequent day, he must paint a new section that is next to a section he has already painted, since he does not want to split up the mural. At the end of each day, one section of the wall will be destroyed. It is always a section of wall that is adjacent to only one other section and is unpainted (Thanh is using a waterproof paint, so painted sections can’t be destroyed). The total beauty of Thanh’s mural will be equal to the sum of the beauty scores of the sections he has painted. Thanh would like to guarantee that, no matter how the wall is destroyed, he can still achieve a total beauty of at least B. What’s the maximum value of B for which he can make this guarantee? . Input . The first line of the input gives the number of test cases, T. T test cases follow. Each test case starts with a line containing an integer N. Then, another line follows containing a string of N digits from 0 to 9. The i-th digit represents the beauty score of the i-th section of the wall. . Output . For each test case, output one line containing Case #x: y, where x is the test case number (starting from 1) and y is the maximum beauty score that Thanh can guarantee that he can achieve, as described above. . Limits . 1 ≤ T ≤ 100. Time limit: 20 seconds per test set. Memory limit: 1 GB. . Small dataset (Test set 1 - Visible) . 2 ≤ N ≤ 100. . Large dataset (Test set 2 - Hidden) . For exactly 1 case, N = 5 × 106; for the other T - 1 cases, 2 ≤ N ≤ 100. Sample . Meaning Input Output . Cases | 4 | Case #1: 6 | . #Sections | 4 | Case #2: 14 | . Beauty scores | 1332 | Case #3: 7 | . #Sections | 4 | Case #4: 31 | . Beauty scores | 9583 |   | . #Sections | 3 |   | . Beauty scores | 616 |   | . #Sections | 10 |   | . Beauty scores | 1029384756 |   | . In the first sample case, Thanh can get a total beauty of 6, no matter how the wall is destroyed. On the first day, he can paint either section of wall with beauty score 3. At the end of the day, either the 1st section or the 4th section will be destroyed, but it does not matter which one. On the second day, he can paint the other section with beauty score 3. . | In the second sample case, Thanh can get a total beauty of 14, by painting the leftmost section of wall (with beauty score 9). The only section of wall that can be destroyed is the rightmost one, since the leftmost one is painted. On the second day, he can paint the second leftmost section with beauty score 5. Then the last unpainted section of wall on the right is destroyed. Note that on the second day, Thanh cannot choose to paint the third section of wall (with beauty score 8), since it is not adjacent to any other painted sections. . | In the third sample case, Thanh can get a total beauty of 7. He begins by painting the section in the middle (with beauty score 1). Whichever section is destroyed at the end of the day, he can paint the remaining wall at the start of the second day. . | . Solution . . Looking at the problem and the sample solutions, it is clear that the painted sections are always contiguous(next to each other as chain link) and the length of the painted sections is ceil(N/2). . The solution to the problem is quite simple in it’s own worth but the challenge is to come up with a efficient solution to solve for large number of sections!! The solution to the problem is max of the rolling sum of roll length ceil(N/2) . If you are familiar with pandas library, the solution is a one line of code if we have all the beauty scores of the wall sections in a pandas Series- . import pandas as pd beauty_scores = pd.read_csv(&#39;input/path/to/file&#39;, delimiter=&#39; &#39;) result = pd.beauty_scores.rolling(math.ceil(N/2)).sum().max() . Read the data . . wall_sections = [] beauty_scores = [] with open(&#39;../inputs/mural_2018_H.txt&#39;) as file: cases = int(file.readline().rstrip()) for case in range(cases): wall_sections.append(int(file.readline().rstrip())) beauty_scores.append(list(map(int, list(file.readline().rstrip())))) . Solve the test case one by one . . We first calculate the roll length and then calculate the roll sums of the beauty scores . def solve_a(sections, bs): if sections % 2 == 0: roll = sections // 2 roll_sums = [sum(bs[i:i+roll]) for i in range(sections - roll + 1)] else: roll = sections // 2 + 1 roll_sums = [sum(bs[i:i+roll+1]) for i in range(sections - roll)] return max(roll_sums) for case in range(cases): result = solve(wall_sections[case], beauty_scores[case]) print(&quot;Case #{}: {}&quot;.format(case + 1, result)) . Let us have a look at the number of operations involved- . roll_windows = N - roll_length | summations = (roll_windows) * roll_length | comparisons = roll_windows | . i.e., O(roll_windows + summations + comparisons) Note: The scales are in log . Improved solution, making use of previous results . Based on the operations breakdown we have seen just now, I see that a substantial number of summations can be avoided by utilizing the concept that every successive roll window overlaps the previous roll window except one element/beauty score To save on computations, we just have to add the beauty score of the new section and subtract which we do not want anymore. . Let us have a look at the number of operations involved- . roll_windows = N - roll_length | summations = (roll_windows) * 2 | comparisons = roll_windows | . i.e., O(roll_windows + summations + comparisons) . Based on the graphs it is clear that we see an improvement from O(10^(2log(N))) to O(10^log(N)) . def solve_b(sections, bs): if sections % 2 == 0: roll = sections // 2 tmp = sum(bs[:roll]) max_value = tmp for i in range(1, sections - roll + 1): tmp = tmp + bs[i+roll-1] - bs[i-1] if max_value &lt; tmp: max_value = tmp return max_value else: roll = sections // 2 + 1 tmp = sum(bs[:roll]) max_value = tmp for i in range(1, sections - roll + 1): tmp = tmp + bs[i + roll - 1] - bs[i - 1] if max_value &lt; tmp: max_value = tmp return max_value . Time comparisons . . . Slow(time in seconds) Fast(time in seconds) Improvement factor (times x) Wall sections . 7.91e-05 | 6.48e-05 | 1.22x | 4 | . 4.24e-05 | 3.95e-05 | 1.07x | 4 | . 4.36e-05 | 4.31e-05 | 1.01x | 3 | . 4.38e-05 | 9.08e-05 | 0.48x | 10 | . 0.00121 | 0.00146 | 0.83x | 500 | . 0.0784 | 0.0158 | 4.94x | 5000 | . 6.80 | 0.15 | 44.55x | 50000 | . 712.75 | 1.49 | 478.12x | 500000 | . Try it out . . If you feel like working out or if you have a much simpler and faster approach to solving this problem, I would like to see and learn!! . Here is the link to Test input and the test results if you want to cross check . Case #1: 6 Case #2: 14 Case #3: 7 Case #4: 31 Case #5: 1012 Case #6: 10129 Case #7: 100501 Case #8: 1001276 .",
            "url": "https://logicatcore.github.io/scratchpad/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H.html",
            "relUrl": "/2020/09/10/Google-kickstart-Mural-coding-question-2018-round-H.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Topology constrained search",
            "content": "Topology constrained search . . Objective . . . To find the 2000th such number in the spiral arrangement of hexagons as seen above, which can divide the product of 6 neighbouring numbers perfectly i.e, the number in the central hexagon should be a factor of the product of the adjacent 6 numbers. . (20 * 37 * 19 * 2 * 9 * 21) / 8 = 664335 (20 * 37 * 19 * 2 * 9 * 21) % 8 = 0 . Logic behind the solution . . As there seems to be no pattern among the numbers distribution around any given hexagon which can be leveraged to find the neighbouring numbers of every hexagon. We resort to reproduce the hexagonal arrangement of numbers as per the problem statement in order to actually determine the neighbouring 6 numbers of any number we are interested in. . . | The next important step is to identify the 6 neighbouring numbers of all the numbers based on the euclidean distance of nearest 6 numbers . . | Next, we start to check if the center number is a factor of the product of the 6 neighbouring numbers. Trying to actually multiply and then divide will eventually bring us to the point where a double or float64 can preceisely store the value of multiplication and hence we need to solve this problem in a smart way. My approach to solving this was based on Prime factorisation . First, we find the prime factors of the center number and the surronding numbers | If all the prime factors of the center are present in the prime factors of all the 6 neighbouring numbers. Then the center number is a factor of the 6 other numbers | | Finally, stop the program when the 2000th number meeting our criteria is found . . | .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/31/Akka-Coding-Challenge.html",
            "relUrl": "/2020/08/31/Akka-Coding-Challenge.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Shortest route Dijkstra algorithm implementation",
            "content": "This area is not a drop target. . (Click the drag the nodes to interact with the graph) . Click to animate . Dijikstra’s algorithm . Dijkstra’s algorithm (or Dijkstra’s Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. . This particular coding challenge deals with a single source and single destination. The algorithm states to visit the child planets in the order of least cost and keep updating the path when a route with lower cost is found. . Example: . . Start: at node 1 . Iter: 1 . To vist: [2, 3, 4] in order (children of node 1) | Parent -&gt; [childs] | 2 -&gt; [3, 5] | 3 -&gt; [4, 5, 6] | 4 -&gt; [6] | . iter: 2 . To vist: [[3, 5], [4, 6, 5], [6]] in order (children of unvisited nodes only to avoid repetition) . | Cost of reaching 3 via 2 is 4+1 which is less than 1 -&gt; 3, so update the cost and path to reach 3 . | Cost of reaching 5 via 2 is 4+7 and this is first time we are visiting the node, so store the cost and path to reach 5 . | Cost of reaching 4 via 3 is 4+1+2 and this is less than 1 -&gt; 4, so update the path and cost to reach 4 . | Cost of reaching 6 via 3 is 4+1+4 and this is first time we are visiting the node, so store the cost and path to reach 6 . | Cost of reaching 5 via 3 is now 4+1+5 which is less than 1 -&gt; 3 -&gt; 5 because we have updated the path to reach 3 already and cost of paths 1 -&gt; 2 -&gt; 3 -&gt; 5(new) &lt; 1 -&gt; 2 -&gt; 5(existing path known), so update the cost and path to reach 5 . | Cost of reaching 6 via 4 is 4+1+2+5 and this is more than 1 -&gt; 2 -&gt; 3 -&gt; 6, so do not update the cost and path to reach 6 . | . iter:3 . To visit:[[7], [5, 7]] in order (children of unvisited nodes only to avoid repetition) | Cost of reaching 7 via 5 is 4+1+5+6 and this is first time we are visiting the node, so store the cost and path to reach 7 | Cost of reaching 5 via 6 is 4+1+4+1 and this is same as 1 -&gt; 2 -&gt; 3 -&gt; 5, so do not update the cost and path to reach 5 | Cost of reaching 7 via 6 is 4+1+4+8 and this is more than 1 -&gt; 2 -&gt; 3 -&gt; 5 -&gt; 7, so do not update the cost and path to reach 7 | . Task description . Find the only route from planet Erde to b3-r7-r4nd7 i.e, node 18 to node 246 among 1000 planets and 1500 routes possible in a bidirectional graph/map which allows moving from one planet to another with every route being associated with a cost value. . Working of code . -&gt; Works based on Dijkstra’s algorithm of single source and single destination . -&gt; Planets are visited based on least cost basis. The sub planets are recursively visited after one complete sweep again in least cost basis. . -&gt; Relaxation, current cost, current node, actual path of every planet is kept track in a matrix . -&gt; Execution stops when we reach our destination i.e, planet 246 or the planet named b3-r7-r4nd7 . Result . Path to the destination planet is: [ 18 810 595 132 519 71 432 246] Cost to reach the destination planet is: 2.995687895999458 . {&quot;source&quot;:18,&quot;target&quot;:810,&quot;cost&quot;:0.04060214221510905} {&quot;source&quot;:595,&quot;target&quot;:810,&quot;cost&quot;:0.1628038931266662} {&quot;source&quot;:132,&quot;target&quot;:595,&quot;cost&quot;:0.6331384762650787} {&quot;source&quot;:132,&quot;target&quot;:519,&quot;cost&quot;:0.6333618615566976} {&quot;source&quot;:71,&quot;target&quot;:519,&quot;cost&quot;:0.7625760415706333} {&quot;source&quot;:71,&quot;target&quot;:432,&quot;cost&quot;:0.6742157893614655} {&quot;source&quot;:246,&quot;target&quot;:432,&quot;cost&quot;:0.08898969190380779} . JSON data . Summary . In total there are 1000 Nodes and 1500 bidirectional paths . Every node has the following properties. . Original provided json . &quot;nodes&quot;: [{&quot;label&quot;:&quot;node_0&quot;}] &quot;edges&quot;: [{&quot;source&quot;:343,&quot;target&quot;:801,&quot;cost&quot;:0.8117216039041273}] . Modified json to visualise with sigma . &quot;nodes&quot;: [{&quot;color&quot;: &quot;#000000&quot;, &quot;label&quot;: &quot;0&quot;, &quot;y&quot;: -2.3181617858307746, &quot;x&quot;: 1.2953878183376322, &quot;id&quot;: &quot;0&quot;, &quot;size&quot;: 0.02775471971630339} &quot;edges&quot;: [{&quot;source&quot;:343,&quot;target&quot;:801,&quot;cost&quot;:0.8117216039041273}] . Data types conversion table between JSON &lt;-&gt; Python . | ++-+ | | JSON | Python | | +===============+===================+ | | object | dict | | ++-+ | | array | list | | ++-+ | | string | unicode | | ++-+ | | number (int) | int, long | | ++-+ | | number (real) | float | | ++-+ | | true | True | | ++-+ | | false | False | | ++-+ | +-++ | | Python | JSON | | +===================+===============+ | | dict | object | | +-++ | | list, tuple | array | | +-++ | | str, unicode | string | | +-++ | | int, long, float | number | | +-++ | | True | true | | +-++ | | False | false | | +-++ | | None | null | | +-++ . Address to website . -&gt; https://www.get-in-it.de/magazin/events/challenges/review-coding-in-the-galaxy .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/15/Finding-shortest-route-using-Dijkstra-algorithm.html",
            "relUrl": "/2020/08/15/Finding-shortest-route-using-Dijkstra-algorithm.html",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Finding the coordinates of non-zero pixels in sparse images/matrices",
            "content": "Sparse images/matrices are those in which the contained useful information is less compared to total space being occupied. . To begin with, we will look at an example image/matrix, the output we need and the main take aways from this post. . . Task: To determine the (row, column) values of all the non zero pixels in a matrix or an image. Typical example would be text in a image. If you are familiar with the well known MNIST Handwritten digits dataset, that could be another good example of a sparse image . . The threshold need not be zero and can be any arbitrary value of interest Example images that we will be working with . . . . . We will be going through two approaches in this short tutorial and also see how the methods compare in execution time . Method 1: The traditional and the first approach that comes to mind through for loops . Method 2: We can leverage the broadcasting properties of numpy and find a work around to reach the same result . Libraries used . . matplotlib | time | numpy | . Pre Prep . . First we need to create a sparse image to work with . import time import numpy as np import matplotlib.pyplot as plt # Image height and width dimensions rows = 100 columns = 100 img = np.random.rand(rows, columns) # Drawing number from uniform distribution [0,1] to avoid negative values img = img * 255 img[img &lt; 220] = 0 # this is optional and can be skipped to handle any threshold other than &#39;non zero value&#39; &amp; &#39;zero&#39; img = img.astype(np.uint8) print(f&quot;sparsity: {len(img[img != 0]) * 100 / np.cumproduct(img.shape)[-1]} %&quot;) . Method 1 . . x_coords = np.array([]) # To store column values y_coords = np.array([]) # To store row values start = time.time() for r in range(rows): for c in range(columns): if img[r][c] != 0: x_coords = np.concatenate((x_coords, np.array([c]))) y_coords = np.concatenate((y_coords, np.array([r]))) x_coords = x_coords.reshape(-1, 1) y_coords = y_coords.reshape(-1, 1) coords = np.hstack((y_coords, x_coords)) print(&quot;Finding non zero pixels coordinates with for loops took: &quot;, time.time() - start, &quot; seconds&quot;) . Method 2 . . First we create a template to go with our image dimension and make a boolean mask which we use to find the non zero pixel coordinates . coordinates_grid = np.ones((2, rows, columns), dtype=np.int16) coordinates_grid[0] = coordinates_grid[0] * np.array([range(rows)]).T coordinates_grid[1] = coordinates_grid[1] * np.array([range(rows)]) start = time.time() mask = img != 0 non_zero_coords = np.hstack((coordinates_grid[0][mask].reshape(-1, 1), coordinates_grid[1][mask].reshape(-1, 1))) print(&quot;Finding non zero pixels coordinates using broadcasting took: &quot;, time.time() - start, &quot; seconds&quot;) # print(&quot;Coordinates of the non zero pixels are: n&quot;, non_zero_coords) plt.imshow(img) plt.show() . Results . . sai@sai:~/****/scripts$ python coordinates.py 10 10 information: 13.0 % Finding non zero pixels coordinates with for loops took: 0.0003330707550048828 seconds Finding non zero pixels coordinates using broadcasting took: 3.4809112548828125e-05 seconds sai@sai:~/****/scripts$ python coordinates.py 100 100 information: 13.43 % Finding non zero pixels coordinates with for loops took: 0.024660587310791016 seconds Finding non zero pixels coordinates using broadcasting took: 0.00010442733764648438 seconds sai@sai:~/****/scripts$ python coordinates.py 1000 1000 information: 13.7047 % Finding non zero pixels coordinates with for loops took: 8.874347448348999 seconds Finding non zero pixels coordinates using broadcasting took: 0.007306575775146484 seconds . Conclusions . . It is clear that the execution times differ significantly and the benefits of vectorization becomes more dominant as the input data grows. .",
            "url": "https://logicatcore.github.io/scratchpad/2020/08/13/sparse-image-coordinates.html",
            "relUrl": "/2020/08/13/sparse-image-coordinates.html",
            "date": " • Aug 13, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "Intro . My name is Sai Sharath Kakubal, I am from India and am currently pursuing my Masters course at Technische Hochschule Ingolstadt, Germany in International Automotive Engineering with a specialisation in Vehicle Electronics. . Interests and skills . Having pursued Mechanical Engineering in India and having been part of designing and building a Formula Student Race car during my Bachelor’s in a student team and having gotten opportunites to work as an Intern in a couple of companies has allowed me to hone my skills and build upon them progressively. My interests and skills are spread across a wide spectrum, and I especially like good presentation and like to learn new tools to complete any task in an efficient and an elegant way. . I am also passionate about Machine Learning and A.I fields and keep myself upto date with the latest advancements in these fields. . Programming languages that I have worked with . Python | C++ | Matlab | Java | C | .",
          "url": "https://logicatcore.github.io/scratchpad/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://logicatcore.github.io/scratchpad/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}